{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FNaFFXMw0-B"
      },
      "source": [
        "## Project Header & Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6ehVIEiS_eV"
      },
      "source": [
        "## Setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CY-V2PKsM3iZ"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Environment setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install packages\n",
        "!pip -q install xgboost optuna pvlib properscoring arch\n",
        "\n",
        "# Imports\n",
        "import os, warnings, random\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Stats / time-series\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "\n",
        "# ML\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# XGBoost / HPO\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "\n",
        "# Deep learning (TF/Keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (Dense, LSTM, GRU, Dropout, Bidirectional,\n",
        "                                     Input, LayerNormalization, Attention, MultiHeadAttention)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Solar/metrics/extras\n",
        "import pvlib\n",
        "from scipy import stats\n",
        "from scipy.optimize import minimize\n",
        "from collections import defaultdict\n",
        "import datetime as dt\n",
        "from IPython.display import display\n",
        "\n",
        "# Reproducibility & thread hygiene\n",
        "SEED = 42\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "for var in [\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"VECLIB_MAXIMUM_THREADS\",\"NUMEXPR_NUM_THREADS\"]:\n",
        "    os.environ.setdefault(var, \"1\")\n",
        "\n",
        "# Quick environment report\n",
        "print(\"Environment versions:\")\n",
        "print(f\"  pandas        {pd.__version__}\")\n",
        "print(f\"  numpy         {np.__version__}\")\n",
        "print(f\"  statsmodels   {sm.__version__}\")\n",
        "import sklearn; print(f\"  scikit-learn  {sklearn.__version__}\")\n",
        "print(f\"  xgboost       {xgb.__version__}\")\n",
        "print(f\"  optuna        {optuna.__version__}\")\n",
        "print(f\"  tensorflow    {tf.__version__}\")\n",
        "print(f\"  pvlib         {pvlib.__version__}\")\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "print(\"GPU available:\", bool(gpus), (\"-> \" + gpus[0].name if gpus else \"\"))\n",
        "print(\"All packages imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-6z5qrpvuRz"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7UK1KPnj6Fd"
      },
      "source": [
        "## Loading Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmniD5pyM3iZ"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Data loading & basic checks\n",
        "print(\"Loading Solar Irradiance Dataset...\")\n",
        "\n",
        "DATA_URL = \"https://raw.githubusercontent.com/iamdkshah/Data_Science_Project_23106359/main/solar_data_2024.csv\"\n",
        "FALLBACK_PATH = \"/content/drive/MyDrive/solar-forecasting/data/solar_data_2024.csv\"\n",
        "\n",
        "def standardize_cols(df):\n",
        "    df = df.copy()\n",
        "    df.columns = (df.columns.str.strip()\n",
        "                            .str.replace(r\"[^\\w\\s]+\", \"_\", regex=True)\n",
        "                            .str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "                            .str.lower())\n",
        "    return df\n",
        "\n",
        "def infer_and_set_time_index(df):\n",
        "    candidates = [\"timestamp\",\"datetime\",\"date_time\",\"time\",\"ts\"]\n",
        "    dt_col = next((c for c in candidates if c in df.columns), None)\n",
        "    if dt_col is None:\n",
        "        raise ValueError(\"No datetime-like column found among: \" + \", \".join(candidates))\n",
        "    df[dt_col] = pd.to_datetime(df[dt_col], errors=\"coerce\", utc=True)\n",
        "    df = df.loc[df[dt_col].notna()].sort_values(dt_col)\n",
        "    dup_count = int(df.duplicated(subset=[dt_col]).sum())\n",
        "    df = df.drop_duplicates(subset=[dt_col], keep=\"first\").set_index(dt_col)\n",
        "    return df, dt_col, dup_count\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(DATA_URL, low_memory=False)\n",
        "    print(\"Dataset loaded successfully from GitHub.\")\n",
        "except Exception as e:\n",
        "    print(f\"GitHub load failed: {e}\")\n",
        "    print(\"Attempting local fallback...\")\n",
        "    df = pd.read_csv(FALLBACK_PATH, low_memory=False)\n",
        "    print(\"Dataset loaded from local fallback.\")\n",
        "\n",
        "df = standardize_cols(df)\n",
        "df, dt_col, dup_removed = infer_and_set_time_index(df)\n",
        "\n",
        "# Frequency & gaps\n",
        "deltas = df.index.to_series().diff().dropna()\n",
        "mode_delta = deltas.mode().iloc[0] if not deltas.empty else pd.NaT\n",
        "adherence_30 = float((deltas == pd.Timedelta(minutes=30)).mean()) * 100 if len(deltas) else np.nan\n",
        "gaps = int((deltas > mode_delta).sum()) if pd.notna(mode_delta) else 0\n",
        "\n",
        "print(\"\\nDATASET OVERVIEW\")\n",
        "print(f\"Columns: {len(df.columns)} | Shape: {df.shape} | Mem: {df.memory_usage(deep=True).sum()/1024**2:.2f} MB\")\n",
        "\n",
        "print(\"\\nTEMPORAL COVERAGE\")\n",
        "print(f\"Datetime column: {dt_col}\")\n",
        "print(f\"Start: {df.index.min()}  End: {df.index.max()}\")\n",
        "print(f\"Mode step: {mode_delta}  |  30-min adherence: {adherence_30:.2f}%\")\n",
        "print(f\"Duplicates removed: {dup_removed}  |  Gaps (>{mode_delta}): {gaps}\")\n",
        "\n",
        "print(\"\\nCOLUMN NAMES\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "print(\"\\nFIRST 5 ROWS\")\n",
        "display(df.head(5))\n",
        "\n",
        "print(\"\\nDATA TYPES SUMMARY\")\n",
        "print(df.dtypes.value_counts())\n",
        "\n",
        "targets = [\"ghi\",\"dni\",\"dhi\"]\n",
        "print(\"\\nTARGET VARIABLES STATUS\")\n",
        "for t in targets:\n",
        "    print(f\"{t.upper()}: {'Available' if t in df.columns else 'Missing'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-j5i_gcM3iZ"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Exploratory Data Analysis (EDA) & Data Quality Assessment\n",
        "print(\"Starting Exploratory Data Analysis...\")\n",
        "\n",
        "assert isinstance(df.index, pd.DatetimeIndex), \"df must be indexed by a DatetimeIndex.\"\n",
        "assert df.index.is_monotonic_increasing, \"Index should be sorted chronologically.\"\n",
        "tzinfo = str(df.index.tz) if getattr(df.index, \"tz\", None) is not None else \"naive\"\n",
        "\n",
        "print(\"BASIC DATASET STATISTICS\")\n",
        "print(f\"Rows x Cols: {df.shape[0]} x {df.shape[1]}\")\n",
        "print(f\"Index timezone: {tzinfo}\")\n",
        "print(f\"Start: {df.index.min()}  End: {df.index.max()}\")\n",
        "print(f\"Temporal span: {df.index.max() - df.index.min()}\")\n",
        "\n",
        "# Temporal consistency\n",
        "deltas = df.index.to_series().diff().dropna()\n",
        "mode_delta = deltas.mode().iloc[0] if not deltas.empty else pd.NaT\n",
        "adherence_30 = (deltas == pd.Timedelta(minutes=30)).mean() * 100 if len(deltas) else np.nan\n",
        "dup_count = int(df.index.duplicated(keep=\"first\").sum())\n",
        "gap_count = int((deltas > mode_delta).sum()) if pd.notna(mode_delta) else 0\n",
        "print(\"\\nTEMPORAL CONSISTENCY\")\n",
        "print(f\"Mode step: {mode_delta}  |  30-minute adherence: {adherence_30:.2f}%\")\n",
        "print(f\"Duplicate timestamps: {dup_count}  |  Gaps (>mode step): {gap_count}\")\n",
        "\n",
        "# Missing data\n",
        "missing_cnt = df.isna().sum()\n",
        "missing_pct = (missing_cnt / len(df) * 100).round(2)\n",
        "missing_summary = (pd.DataFrame({\"missing_count\": missing_cnt, \"missing_pct\": missing_pct})\n",
        "                   .sort_values(\"missing_pct\", ascending=False))\n",
        "print(\"\\nMISSING DATA (top 15)\")\n",
        "display(missing_summary.head(15))\n",
        "\n",
        "# Solar variables\n",
        "solar_vars = [\"ghi\", \"dni\", \"dhi\", \"netsolar\", \"totalnet\"]\n",
        "solar_avail = [c for c in solar_vars if c in df.columns]\n",
        "print(\"\\nSOLAR VARIABLES AVAILABLE:\", solar_avail)\n",
        "\n",
        "if solar_avail:\n",
        "    print(\"Solar variables summary\")\n",
        "    display(df[solar_avail].describe().round(2))\n",
        "\n",
        "    # physics checks\n",
        "    phys_summary = {}\n",
        "    for t in solar_avail:\n",
        "        s = pd.to_numeric(df[t], errors=\"coerce\")\n",
        "        phys_summary[t] = {\"negatives\": int((s < 0).sum()),\n",
        "                           \"zeros\": int((s == 0).sum()),\n",
        "                           \"gt_1300\": int((s > 1300).sum())}\n",
        "    print(\"Physical checks {negatives, zeros, >1300 W/m²}\")\n",
        "    for k, v in phys_summary.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    # distributions\n",
        "    to_plot = solar_avail[:4]\n",
        "    if to_plot:\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 9))\n",
        "        axes = axes.flatten()\n",
        "        for i, col in enumerate(to_plot):\n",
        "            data = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "            data = data[data > 0]\n",
        "            if len(data) > 0:\n",
        "                axes[i].hist(data.dropna(), bins=60, edgecolor=\"black\")\n",
        "                axes[i].set_title(f\"{col.upper()} distribution (excluding zeros)\")\n",
        "                axes[i].set_xlabel(f\"{col.upper()} (W/m²)\")\n",
        "                axes[i].set_ylabel(\"Frequency\")\n",
        "            else:\n",
        "                axes[i].text(0.5, 0.5, f\"{col}: no positive values\", ha=\"center\", va=\"center\")\n",
        "                axes[i].set_axis_off()\n",
        "        for j in range(len(to_plot), len(axes)):\n",
        "            axes[j].set_axis_off()\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "# Weather variables\n",
        "weather_vars = [\"temp_air\", \"relative_humidity\", \"wind_speed\", \"pressure\"]\n",
        "weather_avail = [c for c in weather_vars if c in df.columns]\n",
        "print(\"\\nWEATHER VARIABLES AVAILABLE:\", weather_avail)\n",
        "\n",
        "if weather_avail:\n",
        "    print(\"Weather variables summary\")\n",
        "    display(df[weather_avail].describe().round(2))\n",
        "\n",
        "    n = len(weather_avail)\n",
        "    if n:\n",
        "        rows, cols = 2, min(3, n)\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(17, 8))\n",
        "        axes = np.array(axes).reshape(rows, cols)\n",
        "        for i, col in enumerate(weather_avail):\n",
        "            r, c = divmod(i, cols)\n",
        "            ax = axes[r, c]\n",
        "            ax.hist(pd.to_numeric(df[col], errors=\"coerce\").dropna(), bins=60, edgecolor=\"black\")\n",
        "            ax.set_title(col.replace(\"_\",\" \").title())\n",
        "        for k in range(n, rows*cols):\n",
        "            r, c = divmod(k, cols)\n",
        "            axes[r, c].set_axis_off()\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "# Flags\n",
        "flag_cols = [c for c in df.columns if c.endswith(\"_flag\")]\n",
        "print(\"\\nQUALITY FLAGS\")\n",
        "print(f\"Detected: {len(flag_cols)}\")\n",
        "if flag_cols:\n",
        "    top_show = flag_cols[:10]\n",
        "    for fc in top_show:\n",
        "        counts = df[fc].value_counts(dropna=False).sort_index()\n",
        "        non_zero = int((df[fc] != 0).sum())\n",
        "        print(f\"{fc}: unique={len(counts)}  non_zero={non_zero}\")\n",
        "\n",
        "# Daytime patterns\n",
        "if \"ghi\" in df.columns:\n",
        "    tmp = pd.DataFrame(index=df.index)\n",
        "    tmp[\"ghi\"] = pd.to_numeric(df[\"ghi\"], errors=\"coerce\")\n",
        "    if \"solar_zenith\" in df.columns:\n",
        "        tmp[\"is_day\"] = df[\"solar_zenith\"] <= 90\n",
        "    else:\n",
        "        tmp[\"is_day\"] = tmp[\"ghi\"] > 0\n",
        "\n",
        "    tmp[\"hour\"] = tmp.index.hour\n",
        "    tmp[\"month\"] = tmp.index.month\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    hourly = tmp.loc[tmp[\"is_day\"], :].groupby(\"hour\")[\"ghi\"].mean()\n",
        "    plt.plot(hourly.index, hourly.values, marker=\"o\"); plt.title(\"Average daytime GHI by hour\"); plt.grid(alpha=0.3)\n",
        "    plt.xlabel(\"Hour\"); plt.ylabel(\"GHI (W/m²)\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    monthly = tmp.loc[tmp[\"is_day\"], :].groupby(\"month\")[\"ghi\"].mean()\n",
        "    plt.plot(monthly.index, monthly.values, marker=\"s\"); plt.title(\"Average daytime GHI by month\"); plt.grid(alpha=0.3)\n",
        "    plt.xlabel(\"Month\"); plt.ylabel(\"GHI (W/m²)\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# Correlations\n",
        "corr_cols = [c for c in [\"ghi\", \"dni\", \"dhi\", \"temp_air\", \"relative_humidity\",\n",
        "                         \"wind_speed\", \"pressure\", \"solar_zenith\"] if c in df.columns]\n",
        "if len(corr_cols) >= 3:\n",
        "    corr_df = df[corr_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    corr = corr_df.corr(method=\"spearman\")\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(corr, annot=True, fmt=\".2f\", square=True, cbar=True)\n",
        "    plt.title(\"Spearman correlation (solar + weather)\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "quality_report = {\n",
        "    \"rows\": len(df), \"cols\": len(df.columns), \"tz\": tzinfo,\n",
        "    \"start\": str(df.index.min()), \"end\": str(df.index.max()),\n",
        "    \"mode_step\": str(mode_delta),\n",
        "    \"adherence_30min_pct\": None if np.isnan(adherence_30) else round(adherence_30, 2),\n",
        "    \"duplicates\": dup_count, \"gaps_gt_mode\": gap_count,\n",
        "    \"missing_cols\": int((missing_cnt > 0).sum()),\n",
        "    \"targets_available\": int(sum(c in df.columns for c in [\"ghi\",\"dni\",\"dhi\"])),\n",
        "}\n",
        "print(\"\\nDATA QUALITY SUMMARY\")\n",
        "for k, v in quality_report.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "print(\"\\nExploratory Data Analysis complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ3xyKciN-iS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSPLrDwAM3ia"
      },
      "outputs": [],
      "source": [
        "# Cell 4 — Data preprocessing (fast, leakage-safe)\n",
        "print(\"Starting Data Preprocessing\")\n",
        "\n",
        "base = df.copy()\n",
        "df_work = base.copy()\n",
        "\n",
        "# 1) Ensure DatetimeIndex (already set in Cell 2, but keep guard)\n",
        "if not isinstance(df_work.index, pd.DatetimeIndex):\n",
        "    raise ValueError(\"Dataframe must be indexed by DatetimeIndex after Cell 2.\")\n",
        "df_work.index = df_work.index.tz_convert(\"UTC\") if df_work.index.tz is not None else df_work.index.tz_localize(\"UTC\")\n",
        "df_work.index.name = 'timestamp'\n",
        "\n",
        "# 2) Useful columns\n",
        "solar_cols   = [c for c in ['ghi','dni','dhi','solar_zenith'] if c in df_work.columns]\n",
        "weather_cols = [c for c in ['temp_air','relative_humidity','wind_speed','pressure'] if c in df_work.columns]\n",
        "\n",
        "# 3) Day/night mask\n",
        "if 'solar_zenith' in df_work.columns:\n",
        "    df_work['is_day'] = (pd.to_numeric(df_work['solar_zenith'], errors='coerce') <= 90).astype(int)\n",
        "else:\n",
        "    # coarse fallback\n",
        "    df_work['is_day'] = (pd.to_numeric(df_work.get('ghi', pd.Series(index=df_work.index, dtype=float)), errors='coerce') > 0).astype(int)\n",
        "\n",
        "# 4) Missing summary before\n",
        "key_vars = ['ghi','dni','dhi','temp_air','relative_humidity','wind_speed','pressure']\n",
        "miss_before = {k:int(pd.to_numeric(df_work[k], errors=\"coerce\").isna().sum()) for k in key_vars if k in df_work.columns}\n",
        "print(\"Missing before:\", miss_before)\n",
        "\n",
        "# 5) Clamp negatives & outliers with physics-aware rules\n",
        "night_mask = df_work['is_day'] == 0\n",
        "for v, upper in [('ghi',1400), ('dni',1200), ('dhi',800)]:\n",
        "    if v in df_work.columns:\n",
        "        s = pd.to_numeric(df_work[v], errors=\"coerce\")\n",
        "        # night: negatives -> 0; keep zeros\n",
        "        s.loc[night_mask] = np.maximum(s.loc[night_mask], 0.0)\n",
        "        # day: small negatives -> 0\n",
        "        day_mask = ~night_mask\n",
        "        s.loc[day_mask & (s < 0) & (s > -10)] = 0.0\n",
        "        df_work[v] = s.clip(lower=0, upper=upper)\n",
        "\n",
        "# 6) Impute missing values\n",
        "# solar: time interpolation (<= 3 hours), ok pre-split since it's physical continuity\n",
        "for v in ['ghi','dni','dhi']:\n",
        "    if v in df_work.columns:\n",
        "        df_work[v] = pd.to_numeric(df_work[v], errors=\"coerce\")\n",
        "        df_work[v] = df_work[v].interpolate(method='time', limit=6)\n",
        "\n",
        "# weather: short ffill/bfill then linear (bounded)\n",
        "for v in ['temp_air','relative_humidity','wind_speed','pressure']:\n",
        "    if v in df_work.columns:\n",
        "        s = pd.to_numeric(df_work[v], errors=\"coerce\")\n",
        "        s = s.ffill(limit=4).bfill(limit=4).interpolate(limit=12)\n",
        "        if v == 'relative_humidity': s = s.clip(0, 100)\n",
        "        if v == 'wind_speed':        s = s.clip(lower=0)\n",
        "        if v == 'temp_air':          s = s.clip(-50, 60)\n",
        "        if v == 'pressure':          s = s.clip(800, 1100)\n",
        "        df_work[v] = s\n",
        "\n",
        "# 7) Final numeric coercion (keeps names; helps later dtypes)\n",
        "for c in solar_cols + weather_cols:\n",
        "    if c in df_work.columns:\n",
        "        df_work[c] = pd.to_numeric(df_work[c], errors=\"coerce\")\n",
        "\n",
        "# 8) Missing summary after\n",
        "miss_after = {k:int(df_work[k].isna().sum()) for k in key_vars if k in df_work.columns}\n",
        "print(\"Missing after:\", miss_after)\n",
        "\n",
        "# 9) Final clean frames\n",
        "df_clean = df_work.copy()\n",
        "df_model = df_clean[['ghi']\n",
        "                    + [c for c in ['dni','dhi','solar_zenith'] if c in df_clean.columns]\n",
        "                    + [c for c in ['temp_air','relative_humidity','wind_speed','pressure'] if c in df_clean.columns]\n",
        "                    + ['is_day']].copy()\n",
        "\n",
        "print(f\"Total rows: {len(df_clean)}, Total columns: {df_clean.shape[1]}\")\n",
        "display(df_clean.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIGzzQv6M3ia"
      },
      "outputs": [],
      "source": [
        "# Cell 5 — Feature engineering ( 1-hour horizon)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Starting Feature Engineering for 1-hour ahead\")\n",
        "\n",
        "if \"df_clean\" not in globals():\n",
        "    raise RuntimeError(\"df_clean not found. Run preprocessing first.\")\n",
        "\n",
        "df_features = df_clean.copy()\n",
        "if not isinstance(df_features.index, pd.DatetimeIndex):\n",
        "    raise TypeError(\"df_clean must have a DatetimeIndex.\")\n",
        "\n",
        "# --- detect cadence and set 1-hour horizon in steps ---\n",
        "deltas = df_features.index.to_series().diff().dropna()\n",
        "step = deltas.mode().iloc[0] if not deltas.empty else pd.Timedelta(minutes=30)\n",
        "H_STEPS = int(round(pd.Timedelta(hours=1) / step))\n",
        "if H_STEPS < 1: H_STEPS = 1\n",
        "print(f\"Detected step: {step}, using H_STEPS={H_STEPS} for 1-hour ahead\")\n",
        "\n",
        "# Calendar features\n",
        "df_features[\"hour\"]        = df_features.index.hour\n",
        "df_features[\"weekday\"]     = df_features.index.weekday\n",
        "df_features[\"is_weekend\"]  = (df_features[\"weekday\"] >= 5).astype(int)\n",
        "df_features[\"day_of_year\"] = df_features.index.dayofyear\n",
        "\n",
        "# Cyclical encodings\n",
        "df_features[\"hour_sin\"] = np.sin(2*np.pi*df_features[\"hour\"]/24)\n",
        "df_features[\"hour_cos\"] = np.cos(2*np.pi*df_features[\"hour\"]/24)\n",
        "df_features[\"day_sin\"]  = np.sin(2*np.pi*df_features[\"day_of_year\"]/365)\n",
        "df_features[\"day_cos\"]  = np.cos(2*np.pi*df_features[\"day_of_year\"]/365)\n",
        "\n",
        "# Solar geometry derived\n",
        "if \"solar_zenith\" in df_features.columns:\n",
        "    z = df_features[\"solar_zenith\"].astype(float)\n",
        "    df_features[\"cos_zenith\"]      = np.cos(np.radians(z)).clip(0, 1)\n",
        "    df_features[\"solar_elevation\"] = (90 - z).clip(lower=-90, upper=90)\n",
        "else:\n",
        "    df_features[\"cos_zenith\"] = 0.0\n",
        "    df_features[\"solar_elevation\"] = 0.0\n",
        "\n",
        "# Lag/rolling helpers\n",
        "def add_lags(frame: pd.DataFrame, col: str, lags: list[int]) -> None:\n",
        "    if col not in frame.columns: return\n",
        "    for L in lags:\n",
        "        frame[f\"{col}_lag{L}\"] = frame[col].shift(L)\n",
        "\n",
        "def add_rolling(frame: pd.DataFrame, col: str, windows: list[int]) -> None:\n",
        "    if col not in frame.columns: return\n",
        "    for W in windows:\n",
        "        mp = max(2, W // 2)\n",
        "        frame[f\"{col}_roll{W}_mean\"] = frame[col].rolling(W, min_periods=mp).mean()\n",
        "        frame[f\"{col}_roll{W}_std\"]  = frame[col].rolling(W, min_periods=mp).std()\n",
        "\n",
        "# Horizon-aware lag/rolling sets\n",
        "LAG_SET = sorted(set([1, H_STEPS, 2*H_STEPS, 4*H_STEPS, 12]))  # keep a medium lag too\n",
        "ROLL_SET = sorted(set([H_STEPS, 2*H_STEPS, 4*H_STEPS, 12]))\n",
        "\n",
        "# GHI lags & rolls\n",
        "add_lags(df_features, \"ghi\", LAG_SET)\n",
        "add_rolling(df_features, \"ghi\", ROLL_SET)\n",
        "\n",
        "# Light weather lags\n",
        "for w in [\"temp_air\", \"relative_humidity\", \"wind_speed\", \"pressure\"]:\n",
        "    if w in df_features.columns:\n",
        "        df_features[f\"{w}_lag1\"] = df_features[w].shift(1)\n",
        "        df_features[f\"{w}_lag{H_STEPS}\"] = df_features[w].shift(H_STEPS)\n",
        "\n",
        "# Build target for 1-hour ahead\n",
        "TARGET_COL = f\"target_ghi_t+{H_STEPS}\"\n",
        "if \"ghi\" not in df_features.columns:\n",
        "    raise KeyError(\"Column 'ghi' not found for target creation.\")\n",
        "df_features[TARGET_COL] = df_features[\"ghi\"].shift(-H_STEPS)\n",
        "\n",
        "# Feature list\n",
        "EXOG_BASE = [c for c in [\"hour_sin\",\"hour_cos\",\"day_sin\",\"day_cos\",\"cos_zenith\",\"weekday\",\"is_weekend\"]\n",
        "             if c in df_features.columns]\n",
        "\n",
        "LIGHT_ML_FEATS = (\n",
        "    EXOG_BASE\n",
        "    + [c for c in df_features.columns if c.startswith(\"ghi_lag\")]\n",
        "    + [c for c in df_features.columns if c.startswith(\"ghi_roll\")]\n",
        "    + [c for c in df_features.columns if c.endswith(f\"_lag1\") or c.endswith(f\"_lag{H_STEPS}\")]\n",
        ")\n",
        "\n",
        "FEATURE_COLS = sorted(list(dict.fromkeys(LIGHT_ML_FEATS)))\n",
        "\n",
        "print(\"Features and target ready for 1-hour ahead\")\n",
        "print(f\"Total columns in df_features: {df_features.shape[1]}\")\n",
        "print(f\"Number of feature columns:   {len(FEATURE_COLS)}\")\n",
        "print(\"Sample features:\", FEATURE_COLS[:10])\n",
        "print(\"Target column:\", TARGET_COL)\n",
        "\n",
        "try:\n",
        "    display(df_features[FEATURE_COLS + [TARGET_COL]].head(5))\n",
        "except Exception:\n",
        "    print(df_features[FEATURE_COLS + [TARGET_COL]].head(5))\n",
        "\n",
        "# save shared config so later cells can pick it up\n",
        "ML_DATA = ML_DATA if \"ML_DATA\" in globals() else {}\n",
        "ML_DATA.update({\"FEATS\": FEATURE_COLS, \"TARGET_COL\": TARGET_COL, \"H_STEPS\": H_STEPS})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TgqULHgM3ia"
      },
      "outputs": [],
      "source": [
        "# Cell 6 — Time split, leakage-safe scaling for 1-hour ahead\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"Preparing modeling splits for 1-hour ahead\")\n",
        "\n",
        "FEATURE_COLS = ML_DATA[\"FEATS\"]\n",
        "target_col   = ML_DATA[\"TARGET_COL\"]\n",
        "\n",
        "# keep rows with full features and target\n",
        "model_idx = df_features.dropna(subset=FEATURE_COLS + [target_col]).index\n",
        "df_modeling = df_features.loc[model_idx, FEATURE_COLS + [target_col]].copy()\n",
        "\n",
        "# chronological split 70/15/15\n",
        "n = len(df_modeling)\n",
        "i_train = int(0.70 * n)\n",
        "i_val   = int(0.85 * n)\n",
        "\n",
        "train_idx = df_modeling.index[:i_train]\n",
        "val_idx   = df_modeling.index[i_train:i_val]\n",
        "test_idx  = df_modeling.index[i_val:]\n",
        "\n",
        "# scale features on train only\n",
        "scaler_X = StandardScaler()\n",
        "X_train = scaler_X.fit_transform(df_modeling.loc[train_idx, FEATURE_COLS].values)\n",
        "X_val   = scaler_X.transform(df_modeling.loc[val_idx, FEATURE_COLS].values)\n",
        "X_test  = scaler_X.transform(df_modeling.loc[test_idx, FEATURE_COLS].values)\n",
        "\n",
        "y_train = df_modeling.loc[train_idx, target_col].values\n",
        "y_val   = df_modeling.loc[val_idx, target_col].values\n",
        "y_test  = df_modeling.loc[test_idx, target_col].values\n",
        "\n",
        "ML_DATA.update({\n",
        "    \"X_train\": X_train, \"y_train\": y_train,\n",
        "    \"X_val\":   X_val,   \"y_val\":   y_val,\n",
        "    \"X_test\":  X_test,  \"y_test\":  y_test,\n",
        "    \"index\": {\"train\": train_idx, \"val\": val_idx, \"test\": test_idx},\n",
        "    \"scaler_X\": scaler_X\n",
        "})\n",
        "\n",
        "print(\"Modeling frame:\", len(df_modeling), \"rows,\", len(FEATURE_COLS), \"features, target:\", target_col)\n",
        "print(\"Split sizes  train:\", X_train.shape[0], \" val:\", X_val.shape[0], \" test:\", X_test.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azdJ_hA2M3ia"
      },
      "outputs": [],
      "source": [
        "# Cell 7 — Rolling CV (3 folds), persistence baseline for 1-hour ahead\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Building rolling CV and computing persistence baseline for 1-hour ahead\")\n",
        "\n",
        "H = ML_DATA[\"H_STEPS\"]              # steps that equal 1 hour\n",
        "target_col = ML_DATA[\"TARGET_COL\"]\n",
        "\n",
        "def rmse(a, b):\n",
        "    return float(np.sqrt(np.mean((a - b)**2)))\n",
        "\n",
        "def nrmse(a, b):\n",
        "    m = max(np.mean(a), 1e-6)\n",
        "    return rmse(a, b) / m\n",
        "\n",
        "def day_mask_at_target(idx, h):\n",
        "    if \"is_day\" not in df_features.columns:\n",
        "        return np.ones(len(idx), dtype=bool)\n",
        "    return df_features.loc[idx, 'is_day'].shift(-h).fillna(False).to_numpy().astype(bool)\n",
        "\n",
        "# Persistence baseline on test: y_hat = y_t for t+1h\n",
        "idx_test = ML_DATA[\"index\"][\"test\"]\n",
        "y_true_all = df_features.loc[idx_test, target_col].to_numpy()\n",
        "y_now_all  = df_model.loc[idx_test, 'ghi'].to_numpy()\n",
        "y_persist_all = y_now_all.copy()\n",
        "\n",
        "mask_day = day_mask_at_target(idx_test, H)\n",
        "\n",
        "mae_all = np.mean(np.abs(y_true_all - y_persist_all))\n",
        "rmse_all = rmse(y_true_all, y_persist_all)\n",
        "nrmse_all = nrmse(y_true_all, y_persist_all)\n",
        "\n",
        "mae_day = np.mean(np.abs(y_true_all[mask_day] - y_persist_all[mask_day])) if mask_day.any() else np.nan\n",
        "rmse_day = rmse(y_true_all[mask_day], y_persist_all[mask_day]) if mask_day.any() else np.nan\n",
        "nrmse_day = nrmse(y_true_all[mask_day], y_persist_all[mask_day]) if mask_day.any() else np.nan\n",
        "\n",
        "# Directional accuracy relative to y_t\n",
        "true_dir = np.sign(y_true_all - y_now_all)\n",
        "pred_dir = np.sign(y_persist_all - y_now_all)   # mostly zeros for persistence\n",
        "dir_acc = np.mean((true_dir == pred_dir).astype(float))\n",
        "\n",
        "print(\"Persistence baseline on test (t+1h)\")\n",
        "print(f\"ALL  : MAE={mae_all:.2f}, RMSE={rmse_all:.2f}, nRMSE={nrmse_all:.3f}\")\n",
        "print(f\"DAY  : MAE={mae_day:.2f}, RMSE={rmse_day:.2f}, nRMSE={nrmse_day:.3f}\")\n",
        "print(f\"Directional accuracy: {dir_acc:.3f}\")\n",
        "\n",
        "# Rolling expanding CV folds (3 folds, small gap) on all rows\n",
        "all_idx = df_features.index.dropna()\n",
        "n = len(all_idx)\n",
        "folds = []\n",
        "n_test = 1000\n",
        "n_gap = 100\n",
        "for k in range(3):\n",
        "    end_test = n - k * n_test\n",
        "    start_test = max(end_test - n_test, 0)\n",
        "    end_train = max(start_test - n_gap, 0)\n",
        "    if end_train < 5000:\n",
        "        break\n",
        "    folds.append({\n",
        "        \"train_slice\": slice(0, end_train),\n",
        "        \"val_slice\": slice(end_train, start_test),\n",
        "        \"test_slice\": slice(start_test, end_test)\n",
        "    })\n",
        "\n",
        "print(f\"Constructed {len(folds)} CV folds\")\n",
        "for i, f in enumerate(folds, 1):\n",
        "    tr = f[\"train_slice\"]; va = f[\"val_slice\"]; te = f[\"test_slice\"]\n",
        "    print(f\"Fold {i}: train={tr.stop - tr.start}, val={va.stop - va.start}, test={te.stop - te.start}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVNhpEBUPM8w"
      },
      "source": [
        "## SARIMA MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJiC1nB6M3ia"
      },
      "outputs": [],
      "source": [
        "# Cell S — SARIMAX\n",
        "import time, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "FEATS      = ML_DATA[\"FEATS\"]\n",
        "TARGET_COL = ML_DATA[\"TARGET_COL\"]\n",
        "H          = int(ML_DATA[\"H_STEPS\"])\n",
        "MOVE_THRESH = 25.0\n",
        "\n",
        "def day_mask_at_target(idx, h=H):\n",
        "    if \"is_day\" not in df_features.columns:\n",
        "        return np.ones(len(idx), dtype=bool)\n",
        "    return df_features.loc[idx, \"is_day\"].shift(-h).fillna(False).to_numpy().astype(bool)\n",
        "\n",
        "def rmse(a, b): a=np.asarray(a,float); b=np.asarray(b,float); return float(np.sqrt(np.mean((a-b)**2)))\n",
        "\n",
        "def pick_top_exog(dev_df, feat_cols, target_col, k=10):\n",
        "    y = pd.to_numeric(dev_df[target_col], errors=\"coerce\")\n",
        "    pairs = []\n",
        "    for c in feat_cols:\n",
        "        x = pd.to_numeric(dev_df[c], errors=\"coerce\")\n",
        "        m = x.notna() & y.notna()\n",
        "        if m.sum() < 100:\n",
        "            continue\n",
        "        r = np.corrcoef(x[m], y[m])[0,1]\n",
        "        if np.isfinite(r): pairs.append((c, abs(float(r))))\n",
        "    pairs.sort(key=lambda t: t[1], reverse=True)\n",
        "    return [c for c,_ in pairs[:k]]\n",
        "\n",
        "# ---- DEV/TEST (day-only)\n",
        "dev_index_full = ML_DATA[\"index\"][\"train\"].union(ML_DATA[\"index\"][\"val\"])\n",
        "dev_index_day  = dev_index_full[day_mask_at_target(dev_index_full)]\n",
        "test_index     = ML_DATA[\"index\"][\"test\"]\n",
        "test_index_day = test_index[day_mask_at_target(test_index)]\n",
        "\n",
        "dev_df  = df_features.loc[dev_index_day, [TARGET_COL] + FEATS].copy()\n",
        "test_df = df_features.loc[test_index_day, [TARGET_COL] + FEATS].copy()\n",
        "dev_df[\"y_now\"]  = df_model.loc[dev_index_day, \"ghi\"].astype(np.float32).values\n",
        "test_df[\"y_now\"] = df_model.loc[test_index_day, \"ghi\"].astype(np.float32).values\n",
        "\n",
        "dev_df  = dev_df.replace([np.inf,-np.inf], np.nan).dropna(how=\"any\")\n",
        "test_df = test_df.replace([np.inf,-np.inf], np.nan).dropna(how=\"any\")\n",
        "\n",
        "y_dev,  y0_dev = dev_df[TARGET_COL].to_numpy(np.float32),  dev_df[\"y_now\"].to_numpy(np.float32)\n",
        "y_tst,  y0_tst = test_df[TARGET_COL].to_numpy(np.float32), test_df[\"y_now\"].to_numpy(np.float32)\n",
        "\n",
        "exog_cols = pick_top_exog(dev_df, FEATS, TARGET_COL, k=10)\n",
        "X_dev_raw = dev_df[exog_cols].to_numpy(np.float32)\n",
        "X_tst_raw = test_df[exog_cols].to_numpy(np.float32)\n",
        "scaler = StandardScaler()\n",
        "X_dev = scaler.fit_transform(X_dev_raw)\n",
        "X_tst = scaler.transform(X_tst_raw)\n",
        "\n",
        "print(f\"[DEV] {len(dev_df)} rows | [TEST] {len(test_df)} rows | exog: {len(exog_cols)}\")\n",
        "\n",
        "# ---- folds\n",
        "def make_folds(n, n_folds=3, val_frac=0.12, gap=48, min_train=1200):\n",
        "    folds, v = [], max(1, int(n*val_frac)); end = n\n",
        "    for _ in range(n_folds):\n",
        "        sv, et = max(end-v,0),  max(end-v-gap,0)\n",
        "        if et < min_train: break\n",
        "        folds.append((slice(0, et), slice(sv, end)))\n",
        "        end = sv\n",
        "    return list(reversed(folds))\n",
        "\n",
        "folds = make_folds(len(y_dev), n_folds=3)\n",
        "\n",
        "# ---- tiny grid\n",
        "pdq_list      = [(1,0,1), (2,0,1)]\n",
        "seasonal_list = [(1,0,1,48)]\n",
        "target_modes  = [\"residual\",\"level\"]\n",
        "\n",
        "def fit_one(y_tr, X_tr, pdq, seasonal, mode):\n",
        "    if mode == \"residual\":\n",
        "        r_tr = (y_tr - y0_dev[:len(y_tr)]).astype(np.float32)\n",
        "        endog, trend = r_tr, \"n\"\n",
        "        pdq = (pdq[0], min(pdq[1],1), pdq[2])\n",
        "    else:\n",
        "        endog, trend = y_tr, \"c\"\n",
        "    m = SARIMAX(endog=endog, exog=X_tr, order=pdq, seasonal_order=seasonal,\n",
        "                trend=trend, simple_differencing=True,\n",
        "                enforce_stationarity=True, enforce_invertibility=True)\n",
        "    return m.fit(disp=False, method=\"lbfgs\", maxiter=200)\n",
        "\n",
        "# ---- select best by DEV RMSE\n",
        "results_grid = []\n",
        "for pdq in pdq_list:\n",
        "    for seas in seasonal_list:\n",
        "        for mode in target_modes:\n",
        "            fold_rm = []\n",
        "            for tr_sl, va_sl in folds:\n",
        "                y_tr, y_va = y_dev[tr_sl], y_dev[va_sl]\n",
        "                X_tr, X_va = X_dev[tr_sl], X_dev[va_sl]\n",
        "                res = fit_one(y_tr, X_tr, pdq, seas, mode)\n",
        "                # FIX: use len(y_va), not len(va_sl)\n",
        "                steps = len(y_va)\n",
        "                yhat_va = np.asarray(res.get_forecast(steps=steps, exog=X_va).predicted_mean, float)\n",
        "                if mode == \"residual\": yhat_va = y0_dev[va_sl] + yhat_va\n",
        "                fold_rm.append(rmse(y_va, yhat_va))\n",
        "            results_grid.append({\"pdq\": pdq, \"seasonal\": seas, \"mode\": mode,\n",
        "                                 \"mean_rmse\": float(np.mean(fold_rm)), \"fold_rmse\": fold_rm})\n",
        "\n",
        "best = min(results_grid, key=lambda d: d[\"mean_rmse\"])\n",
        "print(f\"\\nChosen config by DEV RMSE: pdq={best['pdq']} seasonal={best['seasonal']} mode={best['mode']}\")\n",
        "print(\"Fold RMSE:\", \", \".join(f\"{v:.1f}\" for v in best[\"fold_rmse\"]))\n",
        "\n",
        "# ---- refit on full DEV; test\n",
        "t0 = time.time()\n",
        "final_res = fit_one(y_dev, X_dev, best[\"pdq\"], best[\"seasonal\"], best[\"mode\"])\n",
        "train_time = time.time() - t0\n",
        "\n",
        "t1 = time.time()\n",
        "yhat_tst = np.asarray(final_res.get_forecast(steps=len(y_tst), exog=X_tst).predicted_mean, float)\n",
        "if best[\"mode\"] == \"residual\": yhat_tst = y0_tst + yhat_tst\n",
        "infer_time = time.time() - t1\n",
        "\n",
        "MAE = mean_absolute_error(y_tst, yhat_tst)\n",
        "RMSE = rmse(y_tst, yhat_tst)\n",
        "R2   = r2_score(y_tst, yhat_tst)\n",
        "dir_all = float(np.mean((np.sign(y_tst - y0_tst) == np.sign(yhat_tst - y0_tst)).astype(float)))\n",
        "big = np.abs(y_tst - y0_tst) >= MOVE_THRESH\n",
        "dir_big = float(np.mean((np.sign(y_tst[big] - y0_tst[big]) == np.sign(yhat_tst[big] - y0_tst[big])).astype(float))) if big.any() else np.nan\n",
        "skill_vs_persist = 1.0 - (RMSE / rmse(y_tst, y0_tst))\n",
        "\n",
        "print(f\"\\nSARIMAX (tiny-grid) — Test (day-only)\")\n",
        "print(f\"ALL_MAE: {MAE:.4f}\")\n",
        "print(f\"ALL_RMSE: {RMSE:.4f}\")\n",
        "print(f\"ALL_R2: {R2:.4f}\")\n",
        "print(f\"DIR_ACC (all day): {dir_all:.3f}  |  Dir. accuracy (|Δ|≥{MOVE_THRESH:.0f}): {dir_big:.3f}\")\n",
        "print(f\"Train time: {train_time:.2f}s | Inference time (test): {infer_time:.2f}s\")\n",
        "print(f\"Skill vs persistence (DAY): {skill_vs_persist*100:.1f}%\")\n",
        "\n",
        "# ---- plots\n",
        "plt.figure(figsize=(6.2, 2.9))\n",
        "xs = np.arange(1, len(best[\"fold_rmse\"])+1)\n",
        "plt.plot(xs, best[\"fold_rmse\"], marker=\"o\")\n",
        "plt.title(\"SARIMAX — learning curve (DEV RMSE)\")\n",
        "plt.xlabel(\"expanding fold\"); plt.ylabel(\"RMSE (W/m²)\")\n",
        "plt.grid(alpha=0.3); plt.tight_layout(); plt.show()\n",
        "\n",
        "n_week = min(336, len(y_tst))\n",
        "week_time = test_df.index[:n_week]\n",
        "plt.figure(figsize=(12, 2.9))\n",
        "plt.plot(week_time, y_tst[:n_week], label=\"Actual\", lw=1.4, alpha=0.9)\n",
        "plt.plot(week_time, yhat_tst[:n_week], label=\"SARIMAX\", lw=1.2, alpha=0.9)\n",
        "plt.title(\"SARIMAX: Prediction vs Actual (test week)\")\n",
        "plt.xlabel(\"time\"); plt.ylabel(\"GHI (W/m²)\")\n",
        "plt.legend(); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(5.6, 5.6))\n",
        "plt.scatter(y_tst, yhat_tst, s=20, alpha=0.35)\n",
        "mx = max(float(np.max(y_tst)), float(np.max(yhat_tst)))\n",
        "plt.plot([0, mx], [0, mx], \"k--\", lw=1)\n",
        "plt.title(f\"SARIMAX: Scatter (R²={R2:.3f})\")\n",
        "plt.xlabel(\"Actual GHI (W/m²)\"); plt.ylabel(\"Predicted GHI (W/m²)\")\n",
        "plt.grid(alpha=0.3); plt.tight_layout(); plt.show()\n",
        "\n",
        "# exogenous \"importance\" (real names)\n",
        "coef_map = {}\n",
        "param_names = list(getattr(final_res, \"param_names\", []))\n",
        "params = np.asarray(getattr(final_res, \"params\", []), float)\n",
        "for name, val in zip(param_names, params):\n",
        "    for f in exog_cols:\n",
        "        if f in name:\n",
        "            coef_map[f] = coef_map.get(f, 0.0) + abs(float(val))\n",
        "if not coef_map:\n",
        "    y_for_ols = (y_dev - y0_dev) if best[\"mode\"] == \"residual\" else y_dev\n",
        "    try:\n",
        "        ols_res = sm.OLS(y_for_ols, sm.add_constant(X_dev), hasconst=True).fit()\n",
        "        for j, f in enumerate(exog_cols): coef_map[f] = abs(float(ols_res.params[1+j]))\n",
        "    except Exception: coef_map = {f: 0.0 for f in exog_cols}\n",
        "\n",
        "feat, mag = zip(*sorted(coef_map.items(), key=lambda kv: kv[1], reverse=True))\n",
        "plt.figure(figsize=(6.8, 3.0))\n",
        "plt.barh(feat[::-1], np.array(mag[::-1], float))\n",
        "plt.xlabel(\"[coefficient]\"); plt.title(\"SARIMAX: exogenous coefficient magnitude\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# save record for later comparisons\n",
        "ML_DATA.setdefault(\"models\", {})\n",
        "ML_DATA[\"models\"][\"SARIMAX_benchmark\"] = {\n",
        "    \"config\": {\"pdq\": best[\"pdq\"], \"seasonal\": best[\"seasonal\"], \"mode\": best[\"mode\"], \"exog_cols\": exog_cols},\n",
        "    \"dev_folds_rmse\": best[\"fold_rmse\"],\n",
        "    \"metrics_test\": {\"MAE\": MAE, \"RMSE\": RMSE, \"R2\": R2, \"DIR_ACC_all\": dir_all,\n",
        "                     \"DIR_ACC_big\": dir_big, \"skill_vs_persist\": skill_vs_persist},\n",
        "    \"pred_test\": {\"y_true\": y_tst, \"y_pred\": yhat_tst, \"time\": test_df.index.to_numpy()},\n",
        "}\n",
        "print(\"\\nSaved: ML_DATA['models']['SARIMAX_benchmark'].\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOuANC54SqCf"
      },
      "source": [
        "##  XGBOOST: Baseline and Tuned/Optimised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F19ibXW1M3ib"
      },
      "outputs": [],
      "source": [
        "# Cell : XGBoost setup\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Config shorthands\n",
        "H = ML_DATA.get(\"H_STEPS\", 6)\n",
        "TARGET_COL = ML_DATA.get(\"TARGET_COL\", f\"target_ghi_t+{H}\")\n",
        "FEATS = ML_DATA[\"FEATS\"]\n",
        "\n",
        "# General helpers\n",
        "def rmse(a, b):\n",
        "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
        "    return float(np.sqrt(np.mean((a - b)**2)))\n",
        "\n",
        "def mae(a, b):\n",
        "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
        "    return float(np.mean(np.abs(a - b)))\n",
        "\n",
        "def day_mask_for_indices(idx, h=None):\n",
        "    \"\"\"Return boolean mask for whether t+h is daytime.\"\"\"\n",
        "    h = H if h is None else h\n",
        "    if \"is_day\" not in df_features.columns:\n",
        "        return np.ones(len(idx), dtype=bool)\n",
        "    return df_features.loc[idx, \"is_day\"].shift(-h).fillna(False).to_numpy().astype(bool)\n",
        "\n",
        "def predict_best(booster, dmat):\n",
        "    \"\"\"Version-safe predict using best iteration/ntree_limit.\"\"\"\n",
        "    try:\n",
        "        return booster.predict(dmat, iteration_range=(0, booster.best_iteration + 1))\n",
        "    except TypeError:\n",
        "        limit = getattr(booster, \"best_ntree_limit\", 0) or getattr(booster, \"best_iteration\", 0) + 1\n",
        "        return booster.predict(dmat, ntree_limit=limit)\n",
        "\n",
        "# Metrics for report\n",
        "def xgb_compute_metrics(y_true, y_pred, y_now, mask_day):\n",
        "    \"\"\"\n",
        "    y_true: target level at t+H\n",
        "    y_pred: predicted level at t+H\n",
        "    y_now : level at t\n",
        "    mask_day: boolean mask for samples evaluated as 'day' (usually already day-only)\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    y_now  = np.asarray(y_now,  dtype=float)\n",
        "\n",
        "    # All subset passing in\n",
        "    m_all = {\n",
        "        \"ALL_MAE\":  mae(y_true, y_pred),\n",
        "        \"ALL_RMSE\": rmse(y_true, y_pred),\n",
        "        \"ALL_R2\":   float(r2_score(y_true, y_pred)) if len(y_true) > 1 else np.nan,\n",
        "    }\n",
        "\n",
        "    # DAY: using mask_day\n",
        "    if mask_day is None or not np.any(mask_day):\n",
        "        m_day = {\"DAY_MAE\": np.nan, \"DAY_RMSE\": np.nan, \"DAY_R2\": np.nan, \"DAY_nRMSE\": np.nan}\n",
        "    else:\n",
        "        yt_d, yp_d = y_true[mask_day], y_pred[mask_day]\n",
        "        m_day = {\n",
        "            \"DAY_MAE\":  mae(yt_d, yp_d),\n",
        "            \"DAY_RMSE\": rmse(yt_d, yp_d),\n",
        "            \"DAY_R2\":   float(r2_score(yt_d, yp_d)) if len(yt_d) > 1 else np.nan,\n",
        "            \"DAY_nRMSE\": rmse(yt_d, yp_d) / max(np.mean(yt_d), 1e-6)\n",
        "        }\n",
        "\n",
        "    # Directional accuracy (sign of change vs y_t)\n",
        "    true_dir = np.sign(y_true - y_now)\n",
        "    pred_dir = np.sign(y_pred - y_now)\n",
        "    dir_acc = float(np.mean((true_dir == pred_dir).astype(float)))\n",
        "\n",
        "    return {**m_all, **m_day, \"DIR_ACC\": dir_acc}\n",
        "\n",
        "# Plotting\n",
        "def _extract_curve(evals_result, dataset, candidates):\n",
        "    \"\"\"Find the first available metric series in evals_result for a dataset.\"\"\"\n",
        "    series = None\n",
        "    if isinstance(evals_result, dict) and dataset in evals_result:\n",
        "        d = evals_result[dataset]\n",
        "        for name in candidates:\n",
        "            if name in d:\n",
        "                series = d[name]\n",
        "                break\n",
        "    if series is None:\n",
        "        for name in candidates:\n",
        "            if name in evals_result:\n",
        "                sub = evals_result[name]\n",
        "                if dataset in sub:\n",
        "                    series = sub[dataset]\n",
        "                    break\n",
        "    return series\n",
        "\n",
        "def plot_xgb_learning_curves(evals_result, title=\"Learning curves\"):\n",
        "    rmse_tr = _extract_curve(evals_result, \"train\", [\"rmse\", \"l2\"])\n",
        "    rmse_va = _extract_curve(evals_result, \"valid\", [\"rmse\", \"l2\"]) or \\\n",
        "              _extract_curve(evals_result, \"validation_0\", [\"rmse\", \"l2\"])  # fallback names\n",
        "    mae_tr  = _extract_curve(evals_result, \"train\", [\"mae\", \"l1\"])\n",
        "    mae_va  = _extract_curve(evals_result, \"valid\", [\"mae\", \"l1\"]) or \\\n",
        "              _extract_curve(evals_result, \"validation_0\", [\"mae\", \"l1\"])\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 3.8))\n",
        "    if rmse_tr is not None and rmse_va is not None:\n",
        "        axes[0].plot(rmse_tr, label=\"train\")\n",
        "        axes[0].plot(rmse_va, label=\"valid\")\n",
        "        axes[0].set_title(f\"{title} — RMSE\")\n",
        "        axes[0].set_xlabel(\"Boosting rounds\"); axes[0].set_ylabel(\"RMSE\")\n",
        "        axes[0].legend()\n",
        "\n",
        "    if mae_tr is not None and mae_va is not None:\n",
        "        axes[1].plot(mae_tr, label=\"train\")\n",
        "        axes[1].plot(mae_va, label=\"valid\")\n",
        "        axes[1].set_title(f\"{title} — MAE\")\n",
        "        axes[1].set_xlabel(\"Boosting rounds\"); axes[1].set_ylabel(\"MAE\")\n",
        "        axes[1].legend()\n",
        "\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_pred_vs_actual(idx, y_true, y_pred, title=\"Prediction vs Actual\"):\n",
        "    t = pd.Index(idx)\n",
        "    plt.figure(figsize=(12, 3.6))\n",
        "    plt.plot(t, y_true, label=\"Actual\")\n",
        "    plt.plot(t, y_pred, label=\"Pred\")\n",
        "    plt.title(title); plt.xlabel(\"Time\"); plt.ylabel(\"GHI (W/m²)\")\n",
        "    plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_scatter(y_true, y_pred, title=\"Scatter\"):\n",
        "    y_true = np.asarray(y_true, dtype=float); y_pred = np.asarray(y_pred, dtype=float)\n",
        "    plt.figure(figsize=(4.8, 4.8))\n",
        "    plt.scatter(y_true, y_pred, s=12, alpha=0.6)\n",
        "    # 45° line\n",
        "    m = max(np.max(y_true), np.max(y_pred))\n",
        "    mn = min(np.min(y_true), np.min(y_pred))\n",
        "    plt.plot([mn, m], [mn, m])\n",
        "    plt.title(title); plt.xlabel(\"Actual\"); plt.ylabel(\"Predicted\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def make_bins(y, q1=None, q2=None):\n",
        "    \"\"\"Return (q1, q2) tertiles to build Low/Med/High bins.\"\"\"\n",
        "    y = np.asarray(y, dtype=float)\n",
        "    return (np.quantile(y, 1/3.0) if q1 is None else q1,\n",
        "            np.quantile(y, 2/3.0) if q2 is None else q2)\n",
        "\n",
        "def _bin3(arr, q1, q2):\n",
        "    out = np.empty_like(arr, dtype=int)\n",
        "    out[arr < q1] = 0\n",
        "    out[(arr >= q1) & (arr < q2)] = 1\n",
        "    out[arr >= q2] = 2\n",
        "    return out\n",
        "\n",
        "def plot_confusion(y_true, y_pred, q1, q2, title=\"Confusion (L/M/H bins)\"):\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    yt = _bin3(y_true, q1, q2)\n",
        "    yp = _bin3(y_pred, q1, q2)\n",
        "    C = np.zeros((3,3), dtype=int)\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            C[i,j] = int(np.sum((yt==i) & (yp==j)))\n",
        "\n",
        "    plt.figure(figsize=(4.6, 3.6))\n",
        "    plt.imshow(C, cmap=\"viridis\")\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            plt.text(j, i, str(C[i,j]), ha=\"center\", va=\"center\", color=\"w\" if C[i,j]>C.max()/2 else \"k\")\n",
        "    plt.xticks([0,1,2], [\"Low\",\"Med\",\"High\"])\n",
        "    plt.yticks([0,1,2], [\"Low\",\"Med\",\"High\"])\n",
        "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "    plt.title(title)\n",
        "    cb = plt.colorbar(); cb.ax.set_ylabel(\"count\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_gain_importance(booster, top_k=25, title=\"Feature importance (gain)\"):\n",
        "    # booster.get_score returns dict {feature: score}\n",
        "    gain = booster.get_score(importance_type=\"gain\")\n",
        "    if not gain:\n",
        "        print(\"No gain stats available from booster.\")\n",
        "        return\n",
        "    items = sorted(gain.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "    names, vals = zip(*items)\n",
        "    y = np.arange(len(names))[::-1]\n",
        "    plt.figure(figsize=(8, 10 if top_k>20 else 6))\n",
        "    plt.barh(y, vals)\n",
        "    plt.yticks(y, names)\n",
        "    plt.xlabel(\"gain\"); plt.title(title)\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# Permutation importance\n",
        "def permutation_importance_tabular(booster, X_df, y_true, baseline_rmse=None,\n",
        "                                   feat_names=None, sample_cap=3000, random_state=42):\n",
        "    \"\"\"\n",
        "    Compute simple permutation importance by delta-RMSE.\n",
        "    Assumes predictions are LEVEL (not residual). If the model predicts residual,\n",
        "    pass y_pred_level_fn that reconstructs level (handled in calling cell).\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    if sample_cap and len(X_df) > sample_cap:\n",
        "        idx = rng.choice(np.arange(len(X_df)), size=sample_cap, replace=False)\n",
        "        X = X_df.iloc[idx].copy()\n",
        "        y = np.asarray(y_true)[idx]\n",
        "    else:\n",
        "        X = X_df.copy()\n",
        "        y = np.asarray(y_true)\n",
        "\n",
        "    import xgboost as xgb\n",
        "    d = xgb.DMatrix(X, feature_names=FEATS)\n",
        "    y_hat = predict_best(booster, d)\n",
        "    base = rmse(y, y_hat) if baseline_rmse is None else float(baseline_rmse)\n",
        "\n",
        "    res = []\n",
        "    for col in (feat_names or list(X.columns)):\n",
        "        x_copy = X.copy()\n",
        "        x_copy[col] = rng.permutation(x_copy[col].values)\n",
        "        d_shuf = xgb.DMatrix(x_copy, feature_names=FEATS)\n",
        "        y_hat_shuf = predict_best(booster, d_shuf)\n",
        "        delta = rmse(y, y_hat_shuf) - base\n",
        "        res.append((col, float(delta)))\n",
        "    res.sort(key=lambda t: t[1], reverse=True)\n",
        "    return res\n",
        "\n",
        "def plot_perm_importance(importances, title=\"Permutation importance (ΔRMSE)\"):\n",
        "    if not importances:\n",
        "        print(\"No permutation importances to plot.\"); return\n",
        "    names = [n for n,_ in importances]\n",
        "    deltas = [d for _,d in importances]\n",
        "    y = np.arange(len(names))[::-1]\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.barh(y, deltas)\n",
        "    plt.yticks(y, names)\n",
        "    plt.xlabel(\"ΔRMSE vs baseline\"); plt.title(title)\n",
        "    plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miyWTJkvM3ib"
      },
      "outputs": [],
      "source": [
        "# Cell — XGBoost baseline\n",
        "\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "\n",
        "FEATS       = ML_DATA[\"FEATS\"]\n",
        "TARGET_COL  = ML_DATA[\"TARGET_COL\"]\n",
        "H           = int(ML_DATA[\"H_STEPS\"])\n",
        "\n",
        "# tiny helpers\n",
        "def day_mask_at_target(idx, h=H):\n",
        "    if \"is_day\" not in df_features.columns:\n",
        "        return np.ones(len(idx), dtype=bool)\n",
        "    return df_features.loc[idx, \"is_day\"].shift(-h).fillna(False).to_numpy().astype(bool)\n",
        "\n",
        "def predict_best(booster, dmat):\n",
        "    try:\n",
        "        return booster.predict(dmat, iteration_range=(0, booster.best_iteration + 1))\n",
        "    except TypeError:\n",
        "        lim = getattr(booster, \"best_ntree_limit\", 0) or getattr(booster, \"best_iteration\", 0) + 1\n",
        "        return booster.predict(dmat, ntree_limit=lim)\n",
        "\n",
        "# train/val from Cell 6 data split\n",
        "tr_idx = ML_DATA[\"index\"][\"train\"]\n",
        "va_idx = ML_DATA[\"index\"][\"val\"]\n",
        "te_idx = ML_DATA[\"index\"][\"test\"]\n",
        "\n",
        "Xtr = df_features.loc[tr_idx, FEATS].astype(np.float32)\n",
        "Xva = df_features.loc[va_idx, FEATS].astype(np.float32)\n",
        "ytr = df_features.loc[tr_idx, TARGET_COL].astype(np.float32).to_numpy()\n",
        "yva = df_features.loc[va_idx, TARGET_COL].astype(np.float32).to_numpy()\n",
        "\n",
        "# drop any inf/nan rows\n",
        "mask_tr = np.isfinite(ytr) & Xtr.replace([np.inf, -np.inf], np.nan).notna().all(axis=1).to_numpy()\n",
        "mask_va = np.isfinite(yva) & Xva.replace([np.inf, -np.inf], np.nan).notna().all(axis=1).to_numpy()\n",
        "Xtr, ytr = Xtr.iloc[mask_tr], ytr[mask_tr]\n",
        "Xva, yva = Xva.iloc[mask_va], yva[mask_va]\n",
        "\n",
        "dtr = xgb.DMatrix(Xtr, label=ytr, feature_names=FEATS)\n",
        "dva = xgb.DMatrix(Xva, label=yva, feature_names=FEATS)\n",
        "\n",
        "# model & training\n",
        "tree_method = \"gpu_hist\" if tf.config.list_physical_devices('GPU') else \"hist\"\n",
        "params_base = {\n",
        "    \"objective\": \"reg:squarederror\",\n",
        "    \"eval_metric\": [\"rmse\", \"mae\"],\n",
        "    \"tree_method\": tree_method,\n",
        "    \"seed\": SEED,\n",
        "    \"eta\": 0.05,\n",
        "    \"max_depth\": 7,\n",
        "    \"min_child_weight\": 5.0,\n",
        "    \"subsample\": 0.8,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"gamma\": 0.0,\n",
        "    \"lambda\": 2.0,\n",
        "    \"alpha\": 0.0,\n",
        "}\n",
        "num_boost_round = 2500\n",
        "early_stopping_rounds = 100\n",
        "evals_result_base = {}\n",
        "\n",
        "_ = xgb.train(\n",
        "    params_base,\n",
        "    dtr,\n",
        "    num_boost_round=num_boost_round,\n",
        "    evals=[(dtr, \"train\"), (dva, \"valid\")],\n",
        "    early_stopping_rounds=early_stopping_rounds,\n",
        "    evals_result=evals_result_base,\n",
        "    verbose_eval=False,\n",
        ")\n",
        "\n",
        "# TEST index\n",
        "# build day-at-target subset\n",
        "te_mask_day = day_mask_at_target(te_idx, H)\n",
        "te_idx_day  = te_idx[te_mask_day]\n",
        "\n",
        "te_tbl = df_features.loc[te_idx_day, FEATS].astype(np.float32).copy()\n",
        "te_tbl[\"y_t\"]  = df_model.loc[te_idx_day, \"ghi\"].astype(np.float32).values\n",
        "te_tbl[\"y_tH\"] = df_features.loc[te_idx_day, TARGET_COL].astype(np.float32).values\n",
        "te_tbl = te_tbl.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how=\"any\")\n",
        "\n",
        "TEST_DAY_INDEX = te_tbl.index\n",
        "\n",
        "\n",
        "# learning curves\n",
        "try:\n",
        "    plot_xgb_learning_curves(evals_result_base, title=\"XGBoost Baseline — Learning curves\")\n",
        "except NameError:\n",
        "    import matplotlib.pyplot as plt\n",
        "    tr = evals_result_base.get(\"train\", {})\n",
        "    va = evals_result_base.get(\"valid\", {})\n",
        "    plt.figure(figsize=(8,4))\n",
        "    if \"rmse\" in tr and \"rmse\" in va:\n",
        "        plt.plot(tr[\"rmse\"], label=\"train RMSE\")\n",
        "        plt.plot(va[\"rmse\"], label=\"valid RMSE\")\n",
        "    if \"mae\" in tr and \"mae\" in va:\n",
        "        plt.plot(tr[\"mae\"], label=\"train MAE\", linestyle=\"--\")\n",
        "        plt.plot(va[\"mae\"], label=\"valid MAE\", linestyle=\"--\")\n",
        "    plt.xlabel(\"Boosting rounds\"); plt.ylabel(\"Metric\"); plt.title(\"XGB Baseline — Learning curves\")\n",
        "    plt.legend(); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49xXsILiM3ib"
      },
      "outputs": [],
      "source": [
        "# Cell: Optuna tuning for XGBoost\n",
        "\n",
        "import optuna, xgboost as xgb, numpy as np, pandas as pd\n",
        "from time import time\n",
        "\n",
        "print(\"Running Optuna for XGBoost tuned model\")\n",
        "\n",
        "# Configuration\n",
        "H            = int(ML_DATA[\"H_STEPS\"])    # forecasting horizon in steps\n",
        "ALPHA_DIR    = 0.15                       # weight for directional-accuracy term\n",
        "MOVE_THRESH  = 25.0                       # W/m² change to count as a “real” ramp\n",
        "N_TRIALS     = 80\n",
        "VAL_FRAC     = 0.12\n",
        "GAP          = 100\n",
        "MIN_TRAIN    = 3000\n",
        "\n",
        "# Helpers\n",
        "def _rmse(a, b):\n",
        "    a = np.asarray(a, float); b = np.asarray(b, float)\n",
        "    return float(np.sqrt(np.mean((a - b) ** 2)))\n",
        "\n",
        "def _dir_acc(y_true, y_pred, y_now, thr=MOVE_THRESH):\n",
        "    delta_t = y_true - y_now\n",
        "    delta_p = y_pred - y_now\n",
        "    mask = np.abs(delta_t) >= thr\n",
        "    if not np.any(mask):  # if nothing to score, be neutral\n",
        "        return 0.5\n",
        "    return float(np.mean(np.sign(delta_t[mask]) == np.sign(delta_p[mask])))\n",
        "\n",
        "def day_mask_at_target(idx, h=H):\n",
        "    if \"is_day\" not in df_features.columns:\n",
        "        return np.ones(len(idx), dtype=bool)\n",
        "    return df_features.loc[idx, \"is_day\"].shift(-h).fillna(False).to_numpy().astype(bool)\n",
        "\n",
        "def predict_best(booster, dmat):\n",
        "    try:\n",
        "        return booster.predict(dmat, iteration_range=(0, booster.best_iteration + 1))\n",
        "    except TypeError:\n",
        "        lim = getattr(booster, \"best_ntree_limit\", 0) or getattr(booster, \"best_iteration\", 0) + 1\n",
        "        return booster.predict(dmat, ntree_limit=lim)\n",
        "\n",
        "def make_expanding_folds(n, n_folds=3, val_frac=VAL_FRAC, gap=GAP, min_train=MIN_TRAIN):\n",
        "    folds, v = [], max(1, int(n * val_frac))\n",
        "    end = n\n",
        "    for _ in range(n_folds):\n",
        "        start_val = max(end - v, 0)\n",
        "        end_train = max(start_val - gap, 0)\n",
        "        if end_train < min_train: break\n",
        "        folds.append((slice(0, end_train), slice(start_val, end)))\n",
        "        end = start_val\n",
        "    return list(reversed(folds))\n",
        "\n",
        "# DEV table (train∪val, day-only, cleaned)\n",
        "FEATS = ML_DATA[\"FEATS\"]\n",
        "TARGET_COL = ML_DATA[\"TARGET_COL\"]\n",
        "\n",
        "dev_idx_full = ML_DATA[\"index\"][\"train\"].union(ML_DATA[\"index\"][\"val\"])\n",
        "dev_idx_day  = dev_idx_full[day_mask_at_target(dev_idx_full)]\n",
        "\n",
        "X_dev = df_features.loc[dev_idx_day, FEATS].astype(np.float32).replace([np.inf, -np.inf], np.nan)\n",
        "y_tH  = df_features.loc[dev_idx_day, TARGET_COL].astype(np.float32)   # y_{t+H}\n",
        "y_t   = df_model.loc[dev_idx_day, \"ghi\"].astype(np.float32)           # y_t\n",
        "\n",
        "finite = X_dev.notna().all(axis=1) & np.isfinite(y_tH) & np.isfinite(y_t)\n",
        "X_dev = X_dev.loc[finite]\n",
        "y_tH  = y_tH.loc[finite]\n",
        "y_t   = y_t.loc[finite]\n",
        "\n",
        "# Ramp-aware weights: upweight rapid changes a bit\n",
        "ramp_now = (np.abs(y_t.values - y_t.shift(1).fillna(method=\"bfill\").values) >= MOVE_THRESH).astype(float)\n",
        "w_dev = (0.2 + 0.8 * (y_t.values / (float(y_t.values.mean()) + 1e-6))).astype(np.float32)\n",
        "w_dev *= (1.0 + 0.25 * ramp_now).astype(np.float32)\n",
        "\n",
        "print(f\"[DEV] rows after cleaning: {len(X_dev)} | features: {X_dev.shape[1]}\")\n",
        "folds = make_expanding_folds(len(X_dev))\n",
        "print(\"CV folds (end_train, start_val, end_val):\",\n",
        "      [(f[0].stop, f[1].start, f[1].stop) for f in folds])\n",
        "\n",
        "# choose tree method\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    tree_method = \"gpu_hist\" if tf.config.list_physical_devices('GPU') else \"hist\"\n",
        "except Exception:\n",
        "    tree_method = \"hist\"\n",
        "\n",
        "# monotone constraints for sun geometry\n",
        "mono_vec = [0] * len(FEATS)\n",
        "for f in (\"cos_zenith\", \"solar_elevation\"):\n",
        "    if f in FEATS:\n",
        "        mono_vec[FEATS.index(f)] = 1\n",
        "\n",
        "# Optuna objective\n",
        "def objective(trial: optuna.Trial):\n",
        "    # try residual vs level training\n",
        "    target_mode = trial.suggest_categorical(\"target_mode\", [\"residual\", \"level\"])\n",
        "    y_label = (y_tH.values - y_t.values).astype(np.float32) if target_mode == \"residual\" else y_tH.values.astype(np.float32)\n",
        "\n",
        "    params = {\n",
        "        \"objective\": trial.suggest_categorical(\"objective\", [\"reg:squarederror\", \"reg:pseudohubererror\"]),\n",
        "        \"eval_metric\": [\"rmse\", \"mae\"],\n",
        "        \"tree_method\": tree_method,\n",
        "        \"seed\": SEED,\n",
        "        \"eta\": trial.suggest_float(\"eta\", 0.01, 0.08, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 9),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 3.0, 10.0),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.70, 1.00),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.70, 1.00),\n",
        "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.60, 1.00),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 3.0),\n",
        "        \"lambda\": trial.suggest_float(\"lambda\", 1.0, 15.0),\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 0.0, 3.0),\n",
        "        \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
        "        \"max_bin\": trial.suggest_int(\"max_bin\", 128, 512),\n",
        "    }\n",
        "    if params[\"grow_policy\"] == \"lossguide\":\n",
        "        params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 64, 512)\n",
        "    # optional monotonicity\n",
        "    use_mono = trial.suggest_categorical(\"use_monotone\", [0, 1])\n",
        "    if use_mono and any(mono_vec):\n",
        "        params[\"monotone_constraints\"] = \"(\" + \",\".join(str(v) for v in mono_vec) + \")\"\n",
        "\n",
        "    # gentle LR decay\n",
        "    eta0 = float(params[\"eta\"])\n",
        "    def _sched(epoch, eta0=eta0):  # keep signature (epoch) >lr\n",
        "        return max(eta0 * (0.997 ** epoch), 1e-4)\n",
        "    lr_decay = xgb.callback.LearningRateScheduler(_sched)\n",
        "\n",
        "    scores = []\n",
        "    for tr_sl, va_sl in folds:\n",
        "        Xtr, Xva = X_dev.iloc[tr_sl], X_dev.iloc[va_sl]\n",
        "        ytr, yva = y_label[tr_sl],   y_label[va_sl]\n",
        "        wtr, wva = w_dev[tr_sl],     w_dev[va_sl]\n",
        "\n",
        "        # safety\n",
        "        good_tr = np.isfinite(ytr); Xtr, ytr, wtr = Xtr.iloc[good_tr], ytr[good_tr], wtr[good_tr]\n",
        "        good_va = np.isfinite(yva); Xva, yva, wva = Xva.iloc[good_va], yva[good_va], wva[good_va]\n",
        "\n",
        "        dtr = xgb.DMatrix(Xtr, label=ytr, weight=wtr, feature_names=FEATS)\n",
        "        dva = xgb.DMatrix(Xva, label=yva, weight=wva, feature_names=FEATS)\n",
        "\n",
        "        booster = xgb.train(\n",
        "            params, dtr,\n",
        "            num_boost_round=3200,\n",
        "            evals=[(dtr, \"train\"), (dva, \"valid\")],\n",
        "            early_stopping_rounds=140,\n",
        "            verbose_eval=False,\n",
        "            callbacks=[lr_decay],\n",
        "        )\n",
        "\n",
        "        # reconstruct to level for scoring\n",
        "        yhat = predict_best(booster, dva)\n",
        "        idx_va = Xva.index\n",
        "        y_true_va = df_features.loc[idx_va, TARGET_COL].to_numpy().astype(np.float32)\n",
        "        y_now_va  = df_model.loc[idx_va, \"ghi\"].to_numpy().astype(np.float32)\n",
        "        y_pred_va = (y_now_va + yhat) if target_mode == \"residual\" else yhat\n",
        "\n",
        "        r = _rmse(y_true_va, y_pred_va)\n",
        "        da = _dir_acc(y_true_va, y_pred_va, y_now_va, thr=MOVE_THRESH)\n",
        "        scores.append(r * (1.0 + ALPHA_DIR * (1.0 - da)))\n",
        "\n",
        "    trial.set_user_attr(\"target_mode\", target_mode)\n",
        "    return float(np.mean(scores))\n",
        "\n",
        "# Run search\n",
        "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
        "pruner  = optuna.pruners.MedianPruner(n_startup_trials=10)\n",
        "study   = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
        "\n",
        "t0 = time()\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
        "opt_time = time() - t0\n",
        "\n",
        "best_params       = study.best_trial.params.copy()\n",
        "best_target_mode  = study.best_trial.user_attrs.get(\"target_mode\", \"residual\")\n",
        "\n",
        "print(\"\\nBest tuned params\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "print(f\"  target_mode: {best_target_mode}\")\n",
        "print(f\"Optuna time: {opt_time:.2f}s\")\n",
        "\n",
        "# simple fold diagnostics for the winning setting\n",
        "def _eval_params(params, target_mode):\n",
        "    out = []\n",
        "    base = {\"eval_metric\": [\"rmse\",\"mae\"], \"tree_method\": tree_method, \"seed\": SEED, **params}\n",
        "    eta0 = float(base.get(\"eta\", 0.03))\n",
        "    def _sched(epoch, eta0=eta0): return max(eta0 * (0.997 ** epoch), 1e-4)\n",
        "    lr_decay = xgb.callback.LearningRateScheduler(_sched)\n",
        "\n",
        "    y_lab = (y_tH.values - y_t.values).astype(np.float32) if target_mode == \"residual\" else y_tH.values.astype(np.float32)\n",
        "    for tr_sl, va_sl in folds:\n",
        "        Xtr, Xva = X_dev.iloc[tr_sl], X_dev.iloc[va_sl]\n",
        "        ytr, yva = y_lab[tr_sl], y_lab[va_sl]\n",
        "        wtr, wva = w_dev[tr_sl], w_dev[va_sl]\n",
        "        dtr = xgb.DMatrix(Xtr, label=ytr, weight=wtr, feature_names=FEATS)\n",
        "        dva = xgb.DMatrix(Xva, label=yva, weight=wva, feature_names=FEATS)\n",
        "        booster = xgb.train(base, dtr, num_boost_round=3200,\n",
        "                            evals=[(dtr,\"train\"),(dva,\"valid\")],\n",
        "                            early_stopping_rounds=140, verbose_eval=False,\n",
        "                            callbacks=[lr_decay])\n",
        "        yhat = predict_best(booster, dva)\n",
        "        idx = Xva.index\n",
        "        yt  = df_features.loc[idx, TARGET_COL].to_numpy().astype(np.float32)\n",
        "        y0  = df_model.loc[idx, \"ghi\"].to_numpy().astype(np.float32)\n",
        "        yp  = y0 + yhat if target_mode == \"residual\" else yhat\n",
        "        out.append((_rmse(yt, yp), _dir_acc(yt, yp, y0)))\n",
        "    return out\n",
        "\n",
        "fold_stats = _eval_params(best_params, best_target_mode)\n",
        "print(\"\\nBest params — per-fold diagnostics:\")\n",
        "for i,(r,da) in enumerate(fold_stats,1):\n",
        "    print(f\"  Fold {i}: RMSE={r:.2f}, DirAcc={da:.3f}\")\n",
        "print(f\"  Mean RMSE={np.mean([r for r,_ in fold_stats]):.2f}, Mean DirAcc={np.mean([da for _,da in fold_stats]):.3f}\")\n",
        "\n",
        "# artifacts\n",
        "BEST_XGB_PARAMS_1H   = best_params\n",
        "BEST_TARGET_MODE_1H  = best_target_mode\n",
        "BEST_MONOTONE_VEC_1H = mono_vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOw9RkdmM3ib"
      },
      "outputs": [],
      "source": [
        "# Cell 16 — Final tuned XGBoost\n",
        "\n",
        "import xgboost as xgb, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "\n",
        "FEATS      = ML_DATA[\"FEATS\"]\n",
        "TARGET_COL = ML_DATA[\"TARGET_COL\"]\n",
        "H          = int(ML_DATA[\"H_STEPS\"])\n",
        "MOVE_THRESH = 25.0\n",
        "\n",
        "# Small helpers\n",
        "def _rmse(a, b): a=np.asarray(a,float); b=np.asarray(b,float); return float(np.sqrt(np.mean((a-b)**2)))\n",
        "def _mae(a, b):  a=np.asarray(a,float); b=np.asarray(b,float); return float(np.mean(np.abs(a-b)))\n",
        "def _r2(y, yhat): y=np.asarray(y,float); yhat=np.asarray(yhat,float); sst=np.sum((y-y.mean())**2); sse=np.sum((y-yhat)**2); return float(1.0 - sse/(sst+1e-12))\n",
        "def day_mask_at_target(idx, h=H):\n",
        "    if \"is_day\" not in df_features.columns: return np.ones(len(idx), dtype=bool)\n",
        "    return df_features.loc[idx, \"is_day\"].shift(-h).fillna(False).to_numpy().astype(bool)\n",
        "def predict_best(booster, dmat):\n",
        "    try:    return booster.predict(dmat, iteration_range=(0, booster.best_iteration + 1))\n",
        "    except TypeError:\n",
        "        lim = getattr(booster, \"best_ntree_limit\", 0) or getattr(booster, \"best_iteration\", 0) + 1\n",
        "        return booster.predict(dmat, ntree_limit=lim)\n",
        "\n",
        "def plot_xgb_learning_curves(evals_result, title=\"XGB Tuned\"):\n",
        "    tr, va = evals_result.get(\"train\", {}), evals_result.get(\"valid\", {})\n",
        "    fig, ax = plt.subplots(1,2, figsize=(12,3.6), sharex=True)\n",
        "    if \"rmse\" in tr and \"rmse\" in va:\n",
        "        ax[0].plot(tr[\"rmse\"], label=\"train\"); ax[0].plot(va[\"rmse\"], label=\"valid\")\n",
        "        ax[0].set_title(f\"{title} — RMSE\"); ax[0].set_xlabel(\"Boosting rounds\"); ax[0].set_ylabel(\"RMSE\"); ax[0].legend()\n",
        "    if \"mae\" in tr and \"mae\" in va:\n",
        "        ax[1].plot(tr[\"mae\"], label=\"train\"); ax[1].plot(va[\"mae\"], label=\"valid\")\n",
        "        ax[1].set_title(f\"{title} — MAE\"); ax[1].set_xlabel(\"Boosting rounds\"); ax[1].set_ylabel(\"MAE\"); ax[1].legend()\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_pred_vs_actual(idx, y_true, y_pred, title=\"\"):\n",
        "    plt.figure(figsize=(12,3.2))\n",
        "    plt.plot(idx, y_true, label=\"Actual\"); plt.plot(idx, y_pred, label=\"Pred\")\n",
        "    plt.title(title); plt.xlabel(\"Time\"); plt.ylabel(\"GHI (W/m²)\"); plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_scatter(y, yp, title=\"\"):\n",
        "    y, yp = np.asarray(y,float), np.asarray(yp,float)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.scatter(y, yp, s=10, alpha=0.5)\n",
        "    mn, mx = np.nanmin(y), np.nanmax(y); plt.plot([mn,mx],[mn,mx], lw=2)\n",
        "    plt.title(title); plt.xlabel(\"Actual\"); plt.ylabel(\"Predicted\"); plt.tight_layout(); plt.show()\n",
        "\n",
        "def make_bins(y):\n",
        "    q1, q2 = np.quantile(np.asarray(y,float), [1/3, 2/3])\n",
        "    return q1, q2\n",
        "\n",
        "def plot_confusion(y_true, y_pred, q1, q2, title=\"\"):\n",
        "    yb = np.digitize(y_true, [q1, q2]); pb = np.digitize(y_pred, [q1, q2])\n",
        "    m = np.zeros((3,3), dtype=int)\n",
        "    for t,p in zip(yb, pb): m[t, p] += 1\n",
        "    import seaborn as sns\n",
        "    plt.figure(figsize=(5.2,4))\n",
        "    sns.heatmap(m, annot=True, fmt=\"d\", cmap=\"viridis\")\n",
        "    plt.title(title); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.xticks([0.5,1.5,2.5],[\"Low\",\"Med\",\"High\"]); plt.yticks([0.5,1.5,2.5],[\"Low\",\"Med\",\"High\"])\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "def plot_gain_importance(booster, top_k=25, title=\"Feature importance (gain)\"):\n",
        "    score = booster.get_score(importance_type=\"gain\")\n",
        "    if not score: return\n",
        "    items = sorted(score.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "    names, vals = zip(*items)\n",
        "    vals = np.array(vals, float)\n",
        "    plt.figure(figsize=(7,8)); plt.barh(range(len(names)), vals); plt.yticks(range(len(names)), names)\n",
        "    plt.gca().invert_yaxis(); plt.xlabel(\"gain\"); plt.title(title); plt.tight_layout(); plt.show()\n",
        "\n",
        "# DEV\n",
        "dev_idx_full = ML_DATA[\"index\"][\"train\"].union(ML_DATA[\"index\"][\"val\"])\n",
        "dev_idx_day  = dev_idx_full[day_mask_at_target(dev_idx_full)]\n",
        "\n",
        "if \"TEST_DAY_INDEX\" in globals():\n",
        "    test_idx_day = TEST_DAY_INDEX\n",
        "else:\n",
        "    te_idx = ML_DATA[\"index\"][\"test\"]\n",
        "    test_idx_day = te_idx[day_mask_at_target(te_idx)]\n",
        "\n",
        "# Build DEV table\n",
        "dev = df_features.loc[dev_idx_day, FEATS].astype(np.float32).copy()\n",
        "dev[\"y_t\"]  = df_model.loc[dev_idx_day, \"ghi\"].astype(np.float32).values\n",
        "dev[\"y_tH\"] = df_features.loc[dev_idx_day, TARGET_COL].astype(np.float32).values\n",
        "dev = dev.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how=\"any\")\n",
        "\n",
        "# residual\n",
        "target_mode = globals().get(\"BEST_TARGET_MODE_1H\", \"residual\")\n",
        "dev[\"y_lab\"] = (dev[\"y_tH\"] - dev[\"y_t\"]).astype(np.float32) if target_mode == \"residual\" else dev[\"y_tH\"].astype(np.float32)\n",
        "\n",
        "# ramp-aware weights again\n",
        "ramp = (np.abs(dev[\"y_t\"].values - pd.Series(dev[\"y_t\"].values).shift(1).fillna(method=\"bfill\").values) >= MOVE_THRESH).astype(float)\n",
        "w_dev = (0.2 + 0.8*(dev[\"y_t\"].values / (float(dev[\"y_t\"].values.mean())+1e-6))).astype(np.float32) * (1.0 + 0.25*ramp).astype(np.float32)\n",
        "\n",
        "X_dev = dev[FEATS].astype(np.float32)\n",
        "y_lab = dev[\"y_lab\"].values.astype(np.float32)\n",
        "\n",
        "# TEST table\n",
        "te  = df_features.loc[test_idx_day, FEATS].astype(np.float32).copy()\n",
        "te[\"y_t\"]  = df_model.loc[test_idx_day, \"ghi\"].astype(np.float32).values\n",
        "te[\"y_tH\"] = df_features.loc[test_idx_day, TARGET_COL].astype(np.float32).values\n",
        "te = te.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how=\"any\")\n",
        "X_te = te[FEATS].astype(np.float32); y_t_te = te[\"y_t\"].values.astype(np.float32); y_tH_te = te[\"y_tH\"].values.astype(np.float32)\n",
        "\n",
        "print(f\"[DEV day-only] {len(X_dev)} rows \\n[TEST day-only] {len(X_te)} rows\")\n",
        "\n",
        "# Split DEV for early stopping\n",
        "cut = int(0.85 * len(X_dev))\n",
        "Xtr, Xva = X_dev.iloc[:cut], X_dev.iloc[cut:]\n",
        "ytr, yva = y_lab[:cut],     y_lab[cut:]\n",
        "wtr, wva = w_dev[:cut],     w_dev[cut:]\n",
        "\n",
        "dtr = xgb.DMatrix(Xtr, label=ytr, weight=wtr, feature_names=FEATS)\n",
        "dva = xgb.DMatrix(Xva, label=yva, weight=wva, feature_names=FEATS)\n",
        "dte = xgb.DMatrix(X_te, feature_names=FEATS)\n",
        "\n",
        "# Parameters\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    tree_method = \"gpu_hist\" if tf.config.list_physical_devices('GPU') else \"hist\"\n",
        "except Exception:\n",
        "    tree_method = \"hist\"\n",
        "\n",
        "best = dict(globals().get(\"BEST_XGB_PARAMS_1H\", {}))\n",
        "params = {\n",
        "    \"eval_metric\": [\"rmse\",\"mae\"],\n",
        "    \"tree_method\": tree_method,\n",
        "    \"seed\": SEED,\n",
        "    **best\n",
        "}\n",
        "# monotone constraints\n",
        "mono_vec = globals().get(\"BEST_MONOTONE_VEC_1H\", None)\n",
        "if mono_vec and any(mono_vec):\n",
        "    params[\"monotone_constraints\"] = \"(\" + \",\".join(str(int(v)) for v in mono_vec) + \")\"\n",
        "\n",
        "# learning-rate schedule\n",
        "eta0 = float(params.get(\"eta\", 0.03))\n",
        "def _sched(epoch, eta0=eta0): return max(eta0 * (0.997 ** epoch), 1e-4)\n",
        "lr_decay = xgb.callback.LearningRateScheduler(_sched)\n",
        "\n",
        "num_boost_round = 3200\n",
        "esr = 140\n",
        "evals_result = {}\n",
        "\n",
        "# train tuned model\n",
        "t0 = time()\n",
        "booster = xgb.train(\n",
        "    params, dtr, num_boost_round=num_boost_round,\n",
        "    evals=[(dtr,\"train\"), (dva,\"valid\")],\n",
        "    early_stopping_rounds=esr,\n",
        "    evals_result=evals_result,\n",
        "    verbose_eval=False,\n",
        "    callbacks=[lr_decay],\n",
        ")\n",
        "train_time = time() - t0\n",
        "\n",
        "# Predict, reconstruct to level, and clamp\n",
        "t1 = time()\n",
        "yhat_lab = predict_best(booster, dte)\n",
        "y_hat = (y_t_te + yhat_lab) if (params.get(\"objective\",\"reg:squarederror\") and target_mode==\"residual\") else yhat_lab\n",
        "# physics guardrails\n",
        "upper = np.percentile(y_t_te, 99.9)\n",
        "y_hat = np.clip(y_hat, 0.0, upper)\n",
        "infer_time = time() - t1\n",
        "\n",
        "# Metrics\n",
        "rmse = _rmse(y_tH_te, y_hat)\n",
        "mae  = _mae(y_tH_te, y_hat)\n",
        "r2   = _r2(y_tH_te, y_hat)\n",
        "nrmse_mean = rmse / (y_tH_te.mean() + 1e-6)\n",
        "dir_acc = _dir_acc(y_tH_te, y_hat, y_t_te, thr=MOVE_THRESH)\n",
        "\n",
        "print(\"\\n=== XGBoost Tuned — Test (day-only) ===\")\n",
        "print(f\"ALL_MAE: {mae:.4f}\")\n",
        "print(f\"ALL_RMSE: {rmse:.4f}\")\n",
        "print(f\"ALL_R2: {r2:.4f}\")\n",
        "print(f\"DAY_nRMSE: {nrmse_mean:.4f}\")\n",
        "print(f\"DIR_ACC: {dir_acc:.4f}\")\n",
        "print(f\"Train time: {train_time:.2f}s | Inference time (test): {infer_time:.2f}s\")\n",
        "\n",
        "# Plots\n",
        "plot_xgb_learning_curves(evals_result, title=\"XGB Tuned\")\n",
        "plot_pred_vs_actual(X_te.index, y_tH_te, y_hat, title=\"XGB Tuned: Prediction vs Actual (test)\")\n",
        "plot_scatter(y_tH_te, y_hat, title=\"XGB Tuned: Scatter\")\n",
        "\n",
        "q1, q2 = make_bins(dev[\"y_tH\"].values)\n",
        "plot_confusion(y_tH_te, y_hat, q1, q2, title=\"XGB Tuned confusion (L/M/H bins)\")\n",
        "plot_gain_importance(booster, top_k=25, title=\"XGB Tuned — feature importance (gain)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fMuHmNiMnY3"
      },
      "source": [
        "## Hybrid TCN Baseline and optimised"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faY1hED8EmVj"
      },
      "source": [
        "## GRU Baseline and optimised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGWl8LsuM3ic"
      },
      "outputs": [],
      "source": [
        "# Cell GRU 1: GRU baseline\n",
        "\n",
        "import numpy as np, pandas as pd, tensorflow as tf, matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models, callbacks, regularizers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "SEED = int(globals().get(\"SEED\", 2024))\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "FEATS      = ML_DATA[\"FEATS\"]\n",
        "TARGET_COL = ML_DATA[\"TARGET_COL\"]\n",
        "H          = int(ML_DATA[\"H_STEPS\"])           # horizon in steps\n",
        "\n",
        "# Helpers\n",
        "def day_mask_at_target(idx, h=H):\n",
        "    if \"is_day\" not in df_features.columns:\n",
        "        return np.ones(len(idx), dtype=bool)\n",
        "    return df_features.loc[idx, \"is_day\"].shift(-h).fillna(False).to_numpy().astype(bool)\n",
        "\n",
        "def make_seq_arrays(full_df, feat_cols, index_like, lookback):\n",
        "    \"\"\"Builds (X, y, y_now, idx_out) with rolling windows that end at row i.\"\"\"\n",
        "    full_df = full_df.replace([np.inf, -np.inf], np.nan)\n",
        "    arr = full_df[feat_cols].astype(np.float32).to_numpy()\n",
        "    idx_full = full_df.index.to_numpy()\n",
        "    pos = {k:i for i,k in enumerate(idx_full)}\n",
        "\n",
        "    X, y, y_now, idx_out = [], [], [], []\n",
        "    for k in index_like:\n",
        "        i = pos.get(k, None)\n",
        "        if i is None or i - lookback + 1 < 0:\n",
        "            continue\n",
        "        win = arr[i - lookback + 1 : i + 1]\n",
        "        if np.isnan(win).any():\n",
        "            continue\n",
        "        X.append(win)\n",
        "        y.append(float(full_df.at[k, TARGET_COL]))             # y_{t+H}\n",
        "        y_now.append(float(df_model.at[k, \"ghi\"]))              # y_t\n",
        "        idx_out.append(k)\n",
        "    if len(X) == 0:\n",
        "        return np.zeros((0, lookback, len(feat_cols)), np.float32), np.array([]), np.array([]), np.array([])\n",
        "    return np.stack(X), np.array(y, np.float32), np.array(y_now, np.float32), np.array(idx_out)\n",
        "\n",
        "def build_gru(input_shape, units=64, dropout=0.2, l2=0.0):\n",
        "    reg = regularizers.l2(l2) if l2>0 else None\n",
        "    inp = layers.Input(shape=input_shape)\n",
        "    x   = layers.GRU(units, dropout=dropout, recurrent_dropout=0.0,\n",
        "                     kernel_regularizer=reg, recurrent_regularizer=reg, return_sequences=False)(inp)\n",
        "    x   = layers.Dense(units//2, activation=\"relu\", kernel_regularizer=reg)(x)\n",
        "    out = layers.Dense(1, name=\"yhat\")(x)\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-3, clipnorm=1.0),\n",
        "                  loss=\"mse\", metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\"),\n",
        "                                       tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")])\n",
        "    return model\n",
        "\n",
        "# From data split\n",
        "tr_idx = ML_DATA[\"index\"][\"train\"]\n",
        "va_idx = ML_DATA[\"index\"][\"val\"]\n",
        "\n",
        "# Sequences # window\n",
        "LOOKBACK = max(24, H*6)\n",
        "Xtr, ytr, ynow_tr, idx_tr = make_seq_arrays(df_features, FEATS, tr_idx, LOOKBACK)\n",
        "Xva, yva, ynow_va, idx_va = make_seq_arrays(df_features, FEATS, va_idx, LOOKBACK)\n",
        "\n",
        "# scale per feature using train windows only\n",
        "scaler = StandardScaler()\n",
        "Xtr_2d = Xtr.reshape(-1, Xtr.shape[-1])\n",
        "scaler.fit(Xtr_2d)\n",
        "def transform_3d(X):\n",
        "    if X.size == 0: return X\n",
        "    s = X.shape\n",
        "    Z = scaler.transform(X.reshape(-1, s[-1])).reshape(s)\n",
        "    return Z\n",
        "\n",
        "Xtr_s, Xva_s = transform_3d(Xtr), transform_3d(Xva)\n",
        "\n",
        "# build model and train\n",
        "model_base = build_gru(input_shape=(LOOKBACK, len(FEATS)), units=64, dropout=0.2, l2=1e-5)\n",
        "es  = callbacks.EarlyStopping(patience=20, restore_best_weights=True, monitor=\"val_loss\")\n",
        "rlr = callbacks.ReduceLROnPlateau(factor=0.5, patience=8, min_lr=1e-5, monitor=\"val_loss\")\n",
        "\n",
        "hist = model_base.fit(\n",
        "    Xtr_s, ytr,\n",
        "    validation_data=(Xva_s, yva),\n",
        "    epochs=200,\n",
        "    batch_size=256,\n",
        "    verbose=0,\n",
        "    callbacks=[es, rlr],\n",
        ")\n",
        "\n",
        "# Test\n",
        "try:\n",
        "    _ = TEST_DAY_INDEX\n",
        "except NameError:\n",
        "    te_idx = ML_DATA[\"index\"][\"test\"]\n",
        "    mask_day = day_mask_at_target(te_idx, H)\n",
        "    TEST_DAY_INDEX = te_idx[mask_day]\n",
        "\n",
        "# Learning curves\n",
        "plt.figure(figsize=(12,3.2))\n",
        "plt.subplot(1,2,1); plt.plot(hist.history[\"rmse\"], label=\"train\"); plt.plot(hist.history[\"val_rmse\"], label=\"valid\")\n",
        "plt.title(\"GRU Baseline — RMSE\"); plt.xlabel(\"epochs\"); plt.ylabel(\"RMSE\"); plt.legend()\n",
        "plt.subplot(1,2,2); plt.plot(hist.history[\"mae\"], label=\"train\"); plt.plot(hist.history[\"val_mae\"], label=\"valid\")\n",
        "plt.title(\"GRU Baseline — MAE\"); plt.xlabel(\"epochs\"); plt.ylabel(\"MAE\"); plt.legend()\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZHWo_pfM3ic"
      },
      "outputs": [],
      "source": [
        "#Cell G2: Optuna tuning for GRU\n",
        "\n",
        "import time, numpy as np, pandas as pd, optuna, tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, regularizers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Constants\n",
        "TARGET_MODE_CHOICES = (\"level\", \"residual\")\n",
        "LOOKBACK_CHOICES    = (36, 48)\n",
        "UNITS_CHOICES       = (64, 96, 128)\n",
        "LAYERS_CHOICES      = (1, 2)\n",
        "BATCH_CHOICES       = (128, 256)\n",
        "CLIPNORM_CHOICES    = (1.0, 1.5)\n",
        "\n",
        "N_TRIALS    = 40\n",
        "N_FOLDS     = 2\n",
        "MAX_EPOCHS  = 60\n",
        "ES_PATIENCE = 5\n",
        "RLROP_PAT   = 2\n",
        "\n",
        "VAL_FRAC    = 0.12\n",
        "GAP         = 100\n",
        "MIN_TRAIN   = 3000\n",
        "MOVE_THRESH = 25.0\n",
        "ALPHA_DIR   = 0.15\n",
        "\n",
        "SEED = int(globals().get(\"SEED\", 2024))\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "\n",
        "# mixed precision\n",
        "try:\n",
        "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "FEATS      = ML_DATA[\"FEATS\"]\n",
        "TARGET_COL = ML_DATA[\"TARGET_COL\"]\n",
        "H          = int(ML_DATA[\"H_STEPS\"])\n",
        "\n",
        "def day_mask_at_target(idx, h=H):\n",
        "    if \"is_day\" not in df_features.columns:\n",
        "        return np.ones(len(idx), dtype=bool)\n",
        "    return df_features.loc[idx, \"is_day\"].shift(-h).fillna(False).to_numpy().astype(bool)\n",
        "\n",
        "def make_seq_arrays(frame: pd.DataFrame, feat_cols, index_like, lookback: int):\n",
        "    frame = frame.replace([np.inf, -np.inf], np.nan)\n",
        "    mat   = frame[feat_cols].astype(np.float32).to_numpy()\n",
        "    idx_f = frame.index.to_numpy()\n",
        "    pos   = {k:i for i,k in enumerate(idx_f)}\n",
        "    X, yH, y0, out_idx = [], [], [], []\n",
        "    for k in index_like:\n",
        "        i = pos.get(k, None)\n",
        "        if i is None or i - lookback + 1 < 0:\n",
        "            continue\n",
        "        win = mat[i - lookback + 1 : i + 1]\n",
        "        if np.isnan(win).any():\n",
        "            continue\n",
        "        X.append(win)\n",
        "        yH.append(float(frame.at[k, TARGET_COL]))   # y_{t+H}\n",
        "        y0.append(float(df_model.at[k, \"ghi\"]))     # y_t\n",
        "        out_idx.append(k)\n",
        "    if not X:\n",
        "        return (np.zeros((0, lookback, len(feat_cols)), np.float32),\n",
        "                np.array([]), np.array([]), np.array([]))\n",
        "    return np.stack(X), np.array(yH, np.float32), np.array(y0, np.float32), np.array(out_idx)\n",
        "\n",
        "def make_folds(n, n_folds=N_FOLDS, val_frac=VAL_FRAC, gap=GAP, min_train=MIN_TRAIN):\n",
        "    folds = []\n",
        "    v = max(1, int(n * val_frac))\n",
        "    end = n\n",
        "    for _ in range(n_folds):\n",
        "        start_val = max(end - v, 0)\n",
        "        end_train = max(start_val - gap, 0)\n",
        "        if end_train < min_train: break\n",
        "        folds.append((slice(0, end_train), slice(start_val, end)))\n",
        "        end = start_val\n",
        "    return list(reversed(folds))\n",
        "\n",
        "def build_gru(input_shape, units=128, layers_n=1, dropout=0.2, l2=0.0, lr=2e-3, clipnorm=1.0):\n",
        "    reg = regularizers.l2(l2) if l2 > 0 else None\n",
        "    inp = layers.Input(shape=input_shape)\n",
        "    x = layers.GRU(units,\n",
        "                   return_sequences=(layers_n > 1),\n",
        "                   dropout=dropout,\n",
        "                   recurrent_dropout=0.0,    # keep CuDNN fast-path\n",
        "                   kernel_regularizer=reg,\n",
        "                   recurrent_regularizer=reg)(inp)\n",
        "    if layers_n > 1:\n",
        "        x = layers.GRU(units // 2,\n",
        "                       return_sequences=False,\n",
        "                       dropout=dropout,\n",
        "                       recurrent_dropout=0.0,\n",
        "                       kernel_regularizer=reg,\n",
        "                       recurrent_regularizer=reg)(x)\n",
        "    x = layers.Dense(max(units // 2, 32), activation=\"relu\", kernel_regularizer=reg)(x)\n",
        "    out = layers.Dense(1, dtype=\"float32\")(x)  # float32 head keeps metrics stable in mixed precision\n",
        "    model = models.Model(inp, out)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr, clipnorm=clipnorm)\n",
        "    model.compile(optimizer=opt, loss=\"mse\",\n",
        "                  metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\"),\n",
        "                           tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")])\n",
        "    return model\n",
        "\n",
        "def rmse(a, b):\n",
        "    a = np.asarray(a, float); b = np.asarray(b, float)\n",
        "    return float(np.sqrt(np.mean((a - b) ** 2)))\n",
        "\n",
        "# DEV windows\n",
        "dev_idx_full = ML_DATA[\"index\"][\"train\"].union(ML_DATA[\"index\"][\"val\"])\n",
        "dev_idx_day  = dev_idx_full[day_mask_at_target(dev_idx_full)]\n",
        "\n",
        "LOOKBACK_MAX = max(LOOKBACK_CHOICES)   # 48\n",
        "X_all, yH_all, y0_all, idx_all = make_seq_arrays(df_features, FEATS, dev_idx_day, LOOKBACK_MAX)\n",
        "\n",
        "print(f\"[DEV] day-only windows (max L={LOOKBACK_MAX}): {len(X_all)} | features: {X_all.shape[-1]}\")\n",
        "folds = make_folds(len(X_all))\n",
        "print(\"CV folds (end_train, start_val, end_val):\", [(f[0].stop, f[1].start, f[1].stop) for f in folds])\n",
        "\n",
        "# Study\n",
        "sampler = optuna.samplers.TPESampler(seed=SEED, multivariate=True)\n",
        "pruner  = optuna.pruners.SuccessiveHalvingPruner(min_resource=8, reduction_factor=3)\n",
        "STUDY_NAME = f\"gru_1h_fast_{int(time.time())}\"\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner, study_name=STUDY_NAME)\n",
        "\n",
        "def objective(trial: optuna.Trial):\n",
        "    # fixed categorical spaces (tuples) to avoid \"dynamic value space\"\n",
        "    target_mode = trial.suggest_categorical(\"target_mode\", TARGET_MODE_CHOICES)\n",
        "    lookback    = trial.suggest_categorical(\"lookback\", LOOKBACK_CHOICES)\n",
        "    units       = trial.suggest_categorical(\"units\", UNITS_CHOICES)\n",
        "    layers_n    = trial.suggest_categorical(\"layers_n\", LAYERS_CHOICES)\n",
        "    batch_size  = trial.suggest_categorical(\"batch_size\", BATCH_CHOICES)\n",
        "    clipnorm    = trial.suggest_categorical(\"clipnorm\", CLIPNORM_CHOICES)\n",
        "\n",
        "    # continuous ranges\n",
        "    dropout     = trial.suggest_float(\"dropout\", 0.10, 0.35)\n",
        "    l2          = trial.suggest_float(\"l2\", 1e-6, 1e-4, log=True)\n",
        "    lr          = trial.suggest_float(\"lr\", 1e-3, 3e-3, log=True)\n",
        "\n",
        "    fold_scores = []\n",
        "    for tr_sl, va_sl in folds:\n",
        "        Xtr = X_all[tr_sl][:, -lookback:, :]\n",
        "        Xva = X_all[va_sl][:, -lookback:, :]\n",
        "        ytr = yH_all[tr_sl].copy()\n",
        "        yva = yH_all[va_sl].copy()\n",
        "        y0_tr = y0_all[tr_sl]; y0_va = y0_all[va_sl]\n",
        "\n",
        "        if target_mode == \"residual\":\n",
        "            ytr -= y0_tr\n",
        "            yva -= y0_va\n",
        "\n",
        "        # scale features using train only\n",
        "        sc = StandardScaler()\n",
        "        sc.fit(Xtr.reshape(-1, Xtr.shape[-1]))\n",
        "        def T(X):\n",
        "            s = X.shape\n",
        "            return sc.transform(X.reshape(-1, s[-1])).reshape(s)\n",
        "        Xtr_s, Xva_s = T(Xtr), T(Xva)\n",
        "\n",
        "        tf.keras.backend.clear_session()\n",
        "        model = build_gru((lookback, Xtr.shape[-1]), units, layers_n, dropout, l2, lr, clipnorm)\n",
        "\n",
        "        es  = callbacks.EarlyStopping(monitor=\"val_rmse\", patience=ES_PATIENCE,\n",
        "                                      restore_best_weights=True, min_delta=0.5)\n",
        "        rlr = callbacks.ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5,\n",
        "                                          patience=RLROP_PAT, min_lr=5e-5)\n",
        "\n",
        "        model.fit(Xtr_s, ytr, validation_data=(Xva_s, yva),\n",
        "                  epochs=MAX_EPOCHS, batch_size=batch_size, verbose=0,\n",
        "                  callbacks=[es, rlr])\n",
        "\n",
        "        yhat_va = model.predict(Xva_s, verbose=0).reshape(-1)\n",
        "        ypred   = (y0_va + yhat_va) if target_mode == \"residual\" else yhat_va\n",
        "        ytrue   = (yva + y0_va)     if target_mode == \"residual\" else yva\n",
        "\n",
        "        r = rmse(ytrue, ypred)\n",
        "        big = np.abs(ytrue - y0_va) >= MOVE_THRESH\n",
        "        if big.any():\n",
        "            d_true = np.sign(ytrue[big] - y0_va[big])\n",
        "            d_pred = np.sign(ypred[big] - y0_va[big])\n",
        "            dir_acc = float(np.mean((d_true == d_pred).astype(float)))\n",
        "        else:\n",
        "            dir_acc = 0.5\n",
        "\n",
        "        score = r * (1.0 + ALPHA_DIR * (1.0 - dir_acc))\n",
        "        fold_scores.append(score)\n",
        "\n",
        "        trial.report(score, step=len(fold_scores))\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    return float(np.mean(fold_scores))\n",
        "\n",
        "t0 = time.time()\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
        "print(f\"Optuna time: {time.time() - t0:.2f}s\")\n",
        "\n",
        "best_params = study.best_trial.params\n",
        "print(\"\\nBest GRU params\")\n",
        "for k, v in best_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "BEST_GRU_PARAMS_1H = best_params\n",
        "\n",
        "# Save\n",
        "try:\n",
        "    df_trials = study.trials_dataframe()\n",
        "    df_trials.to_csv(\"gru_optuna_trials.csv\", index=False)\n",
        "    print(\"Saved: gru_optuna_trials.csv\")\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpbL8NUcM3ic"
      },
      "outputs": [],
      "source": [
        "# Cell G3 — GRU tuned\n",
        "\n",
        "import time, math, numpy as np, pandas as pd, tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, regularizers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configuration\n",
        "SEED        = int(globals().get(\"SEED\", 2024))\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "try:\n",
        "    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "FEATS      = ML_DATA[\"FEATS\"]\n",
        "TARGET_COL = ML_DATA[\"TARGET_COL\"]\n",
        "H          = int(ML_DATA[\"H_STEPS\"])\n",
        "\n",
        "# Exact day-only test index stored in previous cell\n",
        "def day_mask_at_target(idx, h=H):\n",
        "    if \"is_day\" not in df_features.columns:\n",
        "        return np.ones(len(idx), dtype=bool)\n",
        "    return df_features.loc[idx, \"is_day\"].shift(-h).fillna(False).to_numpy().astype(bool)\n",
        "\n",
        "try:\n",
        "    TEST_DAY_INDEX = TEST_DAY_INDEX\n",
        "except NameError:\n",
        "    te_idx = ML_DATA[\"index\"][\"test\"]\n",
        "    TEST_DAY_INDEX = te_idx[day_mask_at_target(te_idx)]\n",
        "\n",
        "# Best params from G2\n",
        "_g2 = globals().get(\"BEST_GRU_PARAMS_1H\", {})\n",
        "TARGET_MODE = _g2.get(\"target_mode\", \"level\")\n",
        "LOOKBACK    = int(_g2.get(\"lookback\", 48))\n",
        "UNITS       = int(_g2.get(\"units\", 96))\n",
        "LAYERS_N    = int(_g2.get(\"layers_n\", 2))\n",
        "BATCH_SIZE  = int(_g2.get(\"batch_size\", 256))\n",
        "CLIPNORM    = float(_g2.get(\"clipnorm\", 1.0))\n",
        "DROPOUT     = float(_g2.get(\"dropout\", 0.12))\n",
        "L2          = float(_g2.get(\"l2\", 3.8e-06))\n",
        "LR          = float(_g2.get(\"lr\", 1.2e-03))\n",
        "\n",
        "# refining\n",
        "HUBER_DELTA     = 50.0\n",
        "MOVE_THRESH     = 25.0\n",
        "RAMP_WEIGHT     = 1.5\n",
        "ENSEMBLE_SEEDS  = [SEED, SEED + 1]\n",
        "MAX_EPOCHS      = 80\n",
        "ES_PATIENCE     = 8\n",
        "RLROP_PATIENCE  = 3\n",
        "\n",
        "# Helpers\n",
        "def make_seq_arrays(frame: pd.DataFrame, feat_cols, index_like, lookback: int):\n",
        "    \"\"\"Create [N, lookback, F] windows ending at each index entry; returns X, y_{t+H}, y_t, idx.\"\"\"\n",
        "    frame = frame.replace([np.inf, -np.inf], np.nan)\n",
        "    mat   = frame[feat_cols].astype(np.float32).to_numpy()\n",
        "    idx_f = frame.index.to_numpy()\n",
        "    pos   = {k: i for i, k in enumerate(idx_f)}\n",
        "    X, yH, y0, out_idx = [], [], [], []\n",
        "    for k in index_like:\n",
        "        i = pos.get(k, None)\n",
        "        if i is None or i - lookback + 1 < 0:\n",
        "            continue\n",
        "        win = mat[i - lookback + 1 : i + 1]\n",
        "        if np.isnan(win).any():\n",
        "            continue\n",
        "        X.append(win)\n",
        "        yH.append(float(frame.at[k, TARGET_COL]))   # y_{t+H}\n",
        "        y0.append(float(df_model.at[k, \"ghi\"]))     # y_t\n",
        "        out_idx.append(k)\n",
        "    if not X:\n",
        "        return (np.zeros((0, lookback, len(feat_cols)), np.float32),\n",
        "                np.array([]), np.array([]), np.array([]))\n",
        "    return np.stack(X), np.array(yH, np.float32), np.array(y0, np.float32), np.array(out_idx)\n",
        "\n",
        "def rmse(a, b):\n",
        "    a = np.asarray(a, float); b = np.asarray(b, float)\n",
        "    return float(np.sqrt(np.mean((a - b) ** 2))) if len(a) else np.nan\n",
        "\n",
        "def r2_score(y, yhat):\n",
        "    y = np.asarray(y, float); yhat = np.asarray(yhat, float)\n",
        "    ss_res = np.sum((y - yhat) ** 2)\n",
        "    ss_tot = np.sum((y - np.mean(y)) ** 2) + 1e-12\n",
        "    return float(1.0 - ss_res / ss_tot)\n",
        "\n",
        "def build_gru(input_shape, units=128, layers_n=1, dropout=0.2, l2=0.0, lr=2e-3, clipnorm=1.0):\n",
        "    reg = regularizers.l2(l2) if l2 > 0 else None\n",
        "    inp = layers.Input(shape=input_shape)\n",
        "    x = layers.GRU(units,\n",
        "                   return_sequences=(layers_n > 1),\n",
        "                   dropout=dropout,\n",
        "                   recurrent_dropout=0.0,\n",
        "                   kernel_regularizer=reg,\n",
        "                   recurrent_regularizer=reg)(inp)\n",
        "    if layers_n > 1:\n",
        "        x = layers.GRU(max(units // 2, 32),\n",
        "                       return_sequences=False,\n",
        "                       dropout=dropout,\n",
        "                       recurrent_dropout=0.0,\n",
        "                       kernel_regularizer=reg,\n",
        "                       recurrent_regularizer=reg)(x)\n",
        "    x = layers.Dense(max(units // 2, 32), activation=\"relu\", kernel_regularizer=reg)(x)\n",
        "    out = layers.Dense(1, dtype=\"float32\")(x)   # final layer in float32 for numerics\n",
        "    model = models.Model(inp, out)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr, clipnorm=clipnorm)\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss=tf.keras.losses.Huber(delta=HUBER_DELTA),\n",
        "        metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\"),\n",
        "                 tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def standardize_from_train(Xtr, Xva=None, Xte=None):\n",
        "    sc = StandardScaler()\n",
        "    sc.fit(Xtr.reshape(-1, Xtr.shape[-1]))\n",
        "    def T(X):\n",
        "        if X is None: return None\n",
        "        s = X.shape\n",
        "        return sc.transform(X.reshape(-1, s[-1])).reshape(s)\n",
        "    return T(Xtr), T(Xva), T(Xte), sc\n",
        "\n",
        "def ramp_weights(yH, y0, mode):\n",
        "    if mode == \"residual\":\n",
        "        ramps = np.abs(yH) >= MOVE_THRESH\n",
        "    else:\n",
        "        ramps = np.abs(yH - y0) >= MOVE_THRESH\n",
        "    return (1.0 + (RAMP_WEIGHT - 1.0) * ramps.astype(np.float32))\n",
        "\n",
        "# Build DEV and TEST windows\n",
        "dev_idx_full = ML_DATA[\"index\"][\"train\"].union(ML_DATA[\"index\"][\"val\"])\n",
        "dev_idx_day  = dev_idx_full[day_mask_at_target(dev_idx_full)]\n",
        "X_dev, yH_dev, y0_dev, idx_dev = make_seq_arrays(df_features, FEATS, dev_idx_day, LOOKBACK)\n",
        "\n",
        "X_te,  yH_te,  y0_te,  idx_te  = make_seq_arrays(df_features, FEATS, TEST_DAY_INDEX, LOOKBACK)\n",
        "\n",
        "print(f\"[DEV day-only] {len(X_dev)} windows | [TEST day-only] {len(X_te)} windows | F={X_dev.shape[-1] if len(X_dev) else len(FEATS)}\")\n",
        "\n",
        "# chronological 85/15 split on DEV\n",
        "cut = int(0.85 * len(X_dev))\n",
        "Xtr_raw, Xva_raw = X_dev[:cut],   X_dev[cut:]\n",
        "ytr_H,    yva_H   = yH_dev[:cut], yH_dev[cut:]\n",
        "ytr_0,    yva_0   = y0_dev[:cut], y0_dev[cut:]\n",
        "\n",
        "# target transformation\n",
        "if TARGET_MODE == \"residual\":\n",
        "    ytr_lbl = (ytr_H - ytr_0).astype(np.float32)\n",
        "    yva_lbl = (yva_H - yva_0).astype(np.float32)\n",
        "else:\n",
        "    ytr_lbl = ytr_H.astype(np.float32)\n",
        "    yva_lbl = yva_H.astype(np.float32)\n",
        "\n",
        "# standardize\n",
        "Xtr, Xva, Xte_s, scaler = standardize_from_train(Xtr_raw, Xva_raw, X_te)\n",
        "\n",
        "# sample weights\n",
        "w_tr = ramp_weights(ytr_H, ytr_0, TARGET_MODE)\n",
        "\n",
        "# rain 2, seed ensemble\n",
        "histories = []\n",
        "models_ens = []\n",
        "t0_train = time.time()\n",
        "for s in ENSEMBLE_SEEDS:\n",
        "    tf.keras.utils.set_random_seed(int(s))\n",
        "    m = build_gru((LOOKBACK, Xtr.shape[-1]), UNITS, LAYERS_N, DROPOUT, L2, LR, CLIPNORM)\n",
        "    es  = callbacks.EarlyStopping(monitor=\"val_rmse\", patience=ES_PATIENCE,\n",
        "                                  restore_best_weights=True, min_delta=0.3)\n",
        "    rlr = callbacks.ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5,\n",
        "                                      patience=RLROP_PATIENCE, min_lr=5e-5)\n",
        "    h = m.fit(\n",
        "        Xtr, ytr_lbl, validation_data=(Xva, yva_lbl),\n",
        "        epochs=MAX_EPOCHS, batch_size=BATCH_SIZE, verbose=0,\n",
        "        sample_weight=w_tr, callbacks=[es, rlr]\n",
        "    )\n",
        "    histories.append(h.history)\n",
        "    models_ens.append(m)\n",
        "train_time = time.time() - t0_train\n",
        "\n",
        "# learning curve\n",
        "plt.figure(figsize=(8,3.8))\n",
        "plt.plot(histories[0][\"rmse\"], label=\"train RMSE\")\n",
        "plt.plot(histories[0][\"val_rmse\"], label=\"val RMSE\")\n",
        "plt.title(\"GRU : Learning curve\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"RMSE\")\n",
        "plt.legend(); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()\n",
        "\n",
        "# Inference\n",
        "def predict_level(models_list, X, y0, mode):\n",
        "    # average predictions across ensemble and reconstruct to level\n",
        "    preds = []\n",
        "    for m in models_list:\n",
        "        yhat = m.predict(X, verbose=0).reshape(-1)\n",
        "        preds.append(yhat)\n",
        "    yhat_mean = np.mean(np.stack(preds, axis=0), axis=0)\n",
        "    return (y0 + yhat_mean) if mode == \"residual\" else yhat_mean\n",
        "\n",
        "t1 = time.time()\n",
        "y_hat_va = predict_level(models_ens, Xva, yva_0, TARGET_MODE)\n",
        "y_hat_te = predict_level(models_ens, Xte_s, y0_te, TARGET_MODE)\n",
        "infer_time = time.time() - t1\n",
        "\n",
        "# Linear calibration\n",
        "A = np.vstack([y_hat_va, np.ones_like(y_hat_va)]).T\n",
        "a_cal, b_cal = np.linalg.lstsq(A, yva_H, rcond=None)[0]\n",
        "y_hat_te_cal = a_cal * y_hat_te + b_cal\n",
        "\n",
        "# Metrics\n",
        "mask_day = np.ones_like(y_hat_te_cal, dtype=bool)\n",
        "\n",
        "def dir_acc_metrics(y_true, y_pred, y_now):\n",
        "    da_all = float(np.mean(np.sign(y_true - y_now) == np.sign(y_pred - y_now)))\n",
        "    big = np.abs(y_true - y_now) >= MOVE_THRESH\n",
        "    da_big = float(np.mean((np.sign(y_true[big] - y_now[big]) == np.sign(y_pred[big] - y_now[big])).astype(float))) if big.any() else np.nan\n",
        "    return da_all, da_big\n",
        "\n",
        "rmse_te  = rmse(yH_te[mask_day], y_hat_te_cal[mask_day])\n",
        "mae_te   = float(np.mean(np.abs(yH_te[mask_day] - y_hat_te_cal[mask_day])))\n",
        "r2_te    = r2_score(yH_te[mask_day], y_hat_te_cal[mask_day])\n",
        "da_all, da_big = dir_acc_metrics(yH_te, y_hat_te_cal, y0_te)\n",
        "\n",
        "print(\"\\n GRU Tuned/Optimised: \")\n",
        "print(f\"ALL_MAE: {mae_te:.4f}\")\n",
        "print(f\"ALL_RMSE: {rmse_te:.4f}\")\n",
        "print(f\"ALL_R2: {r2_te:.4f}\")\n",
        "print(f\"DIR_ACC (all day): {da_all:.4f}\")\n",
        "print(f\"Dir. accuracy (day, |Δ|≥{MOVE_THRESH:.0f} W/m²): {da_big:.3f}\")\n",
        "print(f\"Train time: {train_time:.2f}s | Inference time (test): {infer_time:.2f}s\")\n",
        "\n",
        "# Skill vs persistence\n",
        "persist_rmse_day = rmse(yH_te[mask_day], y0_te[mask_day])\n",
        "skill_day = 1.0 - (rmse_te / persist_rmse_day) if persist_rmse_day > 0 else np.nan\n",
        "print(f\"Skill vs persistence (DAY): {100*skill_day:.1f}%\")\n",
        "\n",
        "# Plots\n",
        "# Prediction vs Actual\n",
        "plt.figure(figsize=(12,3.2))\n",
        "plt.plot(idx_te, yH_te, label=\"actual\")\n",
        "plt.plot(idx_te, y_hat_te_cal, label=\"pred\")\n",
        "plt.title(\"GRU (refined): Prediction vs Actual (test)\")\n",
        "plt.ylabel(\"GHI (W/m²)\"); plt.xlabel(\"time\")\n",
        "plt.legend(); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()\n",
        "\n",
        "# Scatter with R^2\n",
        "plt.figure(figsize=(4.8,4.8))\n",
        "plt.scatter(yH_te, y_hat_te_cal, s=12, alpha=0.45)\n",
        "lim = [min(yH_te.min(), y_hat_te_cal.min()), max(yH_te.max(), y_hat_te_cal.max())]\n",
        "plt.plot(lim, lim, 'k--', linewidth=1)\n",
        "plt.title(f\"GRU — Scatter (R²={r2_te:.3f})\")\n",
        "plt.xlabel(\"Actual GHI\"); plt.ylabel(\"Predicted GHI\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# Occlusion (permutation) importance\n",
        "def occlusion_importance(models_list, X, y_true, y0, mode, feat_names, repeats=1):\n",
        "    base_pred = predict_level(models_list, X, y0, mode)\n",
        "    base_rmse = rmse(y_true, base_pred)\n",
        "    gains = []\n",
        "    rng = np.random.default_rng(SEED)\n",
        "    for j in range(X.shape[-1]):\n",
        "        rmse_acc = []\n",
        "        for _ in range(repeats):\n",
        "            Xp = X.copy()\n",
        "            # shuffle across samples\n",
        "            for t in range(Xp.shape[1]):\n",
        "                rng.shuffle(Xp[:, t, j])\n",
        "            yp = predict_level(models_list, Xp, y0, mode)\n",
        "            rmse_acc.append(rmse(y_true, yp))\n",
        "        gains.append(np.mean(rmse_acc) - base_rmse)\n",
        "    order = np.argsort(gains)[::-1]\n",
        "    names = [feat_names[k] for k in order]\n",
        "    vals  = [gains[k] for k in order]\n",
        "    return names, vals\n",
        "\n",
        "names, vals = occlusion_importance(models_ens, Xte_s, yH_te, y0_te, TARGET_MODE, FEATS, repeats=1)\n",
        "top = min(20, len(names))\n",
        "plt.figure(figsize=(7.5,5))\n",
        "plt.barh(range(top), vals[:top][::-1])\n",
        "plt.yticks(range(top), names[:top][::-1])\n",
        "plt.xlabel(\"RMSE increase when shuffled (W/m²)\")\n",
        "plt.title(\"GRU: Occlusion (permutation) importance\")\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}